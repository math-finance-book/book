{{< include macros.qmd >}}

# GARCH Models {#sec-c:garch} 

```{python}
#| eval: true
#| echo: false

import plotly
from IPython.display import display, HTML

plotly.offline.init_notebook_mode(connected=True)
display(
    HTML(
        '<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>'
    )
)
```


Thus far, we have assumed that the volatility of the underlying asset is constant or varying in a non-random way during the lifetime of the derivative.  In this chapter we will look at models that relax this assumption and allow the volatility to change randomly.  This is very important, because there is plenty of evidence that volatilities do change over time in a random way.  

In the first three sections, we will consider the problem of estimating the volatility.  The discussion of estimation methods leads naturally into the discussion of modeling a changing volatility.  

## Statistics Review {#sec-s:statistics}

We begin with a brief review of basic statistics.
Given a random sample $\{x_1,\ldots,x_N\}$ of size $N$ from a population with mean $\mu$ and variance $\sigma^2$, the best estimate of $\mu$ is of course the sample mean 
$$\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_i\; .$$
The variance is the expected value of $(x-\mu)^2$, so an obvious estimate of the variance is the sample average of $(x_i-\mu)^2$, replacing $\mu$ with its estimate $\bar{x}$.  This would be
$$\frac{1}{N}\sum_{i=1}^{N} (x_i-\bar{x})^2$$
However, because $\bar{x}$ is computed from the $x_i$, the $x_i$ will deviate less on average from $\bar{x}$ than they do from the true mean $\mu$.  Hence the estimate proposed above will on average be less than $\sigma^2$.  To eliminate this bias, it suffices just to scale the estimate up by a factor of $N/(N-1)$.  This leads to the estimate
$$s^2=\frac{1}{N-1}\sum_{i=1}^{N} (x_i-\bar{x})^2\; ,$$
and the best estimate of $\sigma$ is the square root
$$s=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N} (x_i-\bar{x})^2}\; .$$
To calculate $s^2$, notice that 
\begin{align*}
\sum_{i=1}^{N} (x_i-\bar{x})^2 &= \sum_{i=1}^{N} (x_i^2-2x_i\bar{x}+\bar{x}^2)\\
&=\sum_{i=1}^{N} x_i^2 -2\bar{x}\sum_{i=1}^{N} x_i + \sum_{i=1}^N \bar{x}^2\\
&=\sum_{i=1}^{N} x_i^2 -2\bar{x}(N\bar{x})+N\bar{x}^2\\
&=\sum_{i=1}^{N} x_i^2 -N\bar{x}^2\;.
\end{align*}
Therefore
$$s=\sqrt{\frac{1}{N-1}\left(\sum_{i=1}^{N} x_i^2-N\bar{x}^2\right)}\; .$$

It is important to know how much variation there would be in $\bar{x}$ if one had access to multiple random samples.  More variation means that an $\bar{x}$ computed from a single sample will be a less reliable estimate of $\mu$.  The variance of $\bar{x}$ in repeated samples is $\sigma^2/N$,^[The variance of $\bar{x} = (1/N)(x_1 + \cdots + x_N)$ is, by independence of the $x_i$, equal to $(1/N)^2(\mathrm{var}{x_1} + \cdots + \mathrm{var}{x_N})$, and, because the $x_i$ all have the same variance $\sigma^2$, this is equal to $(1/N)^2 \times N\sigma^2 = \sigma^2/N$.] and our best estimate of this variance is $s^2/N$.  The standard deviation of $\bar{x}$ in repeated samples, which is called the standard error of \index{standard error} $\bar{x}$, is $\sigma/\sqrt{N}$, and we estimate this by $s/\sqrt{N}$, which equals
$$\sqrt{\frac{1}{N(N-1)}\left(\sum_{i=1}^{N} x_i^2-N\bar{x}^2\right)}\; .$$
If the population from which $x$ is sampled has a normal distribution, then a 95\% confidence interval for $\mu$ will be $\bar{x}$ plus or minus 1.96 standard errors.  Even if $x$ does not have a normal distribution, by the Central Limit Theorem, $\bar{x}/\sqrt{N}$ will be approximately normally distributed if the sample size $N$ is large enough, and plus or minus 1.96 standard errors will still be approximately a 95\% confidence interval for $\mu$. \index{confidence interval}

## Estimating a Constant Volatility and Mean {#sec-s:estimatingvolatility}

Consider an asset price that is a geometric Brownian motion under the actual probability measure:
$$\frac{\d  S}{S} = \mu\d   t + \sigma\d   B\; ,$$
where $\mu$ and $\sigma$ are unknown constants and $B$ is a Brownian motion.  We can as usual write this in log form as
$$\d \log S = \left(\mu-\frac{1}{2}\sigma^2\right)\d   t + \sigma\d   B\; .$$
Over a discrete time period of length $\Delta t$, this implies
$$
\Delta \log S = \left(\mu-\frac{1}{2}\sigma^2\right)\Delta t + \sigma \Delta B\;.
$$ {#eq-dlogs}

Suppose we have observed the asset price $S$ at dates $0=t_0<t_1<\cdots< t_N=T$, where $t_i-t_{i-1}=\Delta t$.  If the asset pays dividends, we will take $S$ to be the value of the portfolio in which the dividends are reinvested in new shares.  Thus, in general, $S_{t_i}/S_{t_{i-1}}$ denotes the gross return (one plus the rate of return) between dates $t_{i-1}$ and $t_i$.  This return is measured on a non-compounded and non-annualized basis.  The annualized continuously-compounded rate of return is the rate $r_i$ defined by 
$$\frac{S_{t_i}}{S_{t_{i-1}}} = \mathrm{e}^{r_i\Delta t}\; .$$
This implies that
$$
r_i = \frac{\log S_{t_i}-\log S_{t_{i-1}}}{\Delta t} = \mu-\frac{1}{2}\sigma^2 + \sigma \frac{B_{t_i}-B_{t_{i-1}}}{\Delta t}\;.
$$ {#eq-contcompreturn}

Because $B_{t_i}-B_{t_{i-1}}$ is normally distributed with mean zero and variance $\Delta t$, the sample $\{r_1,\ldots,r_N\}$ is a sample of independent random variables each of which is normally distributed with mean $\mu-\sigma^2/2$ and variance $\sigma^2/\Delta t$.  We are focused on estimating $\sigma^2$, so it will simplify things to define
$$
y_i = r_i\sqrt{\Delta t} = \frac{\log S_{t_i}-\log S_{t_{i-1}}}{\sqrt{\Delta t}}\;.
$$ {#eq-volyi}

The sample $\{y_1,\ldots,y_N\}$ is a sample of independent random variables each of which is normally distributed with mean $(\mu-\sigma^2/2)\sqrt{\Delta t}$ and variance $\sigma^2$.
As was discussed in the previous section, the best estimate of the mean of $y$ is the sample mean
$$\bar{y} = \frac{1}{N}\sum_{i=1}^{N}y_i\; ,$$
and the best estimate of $\sigma^2$ is
$$\hat{\sigma}^2 = \frac{1}{N-1}\sum_{i=1}^{N} (y_i-\bar{y})^2\; .$$
This means that we estimate $\mu$ as
$$\hat{\mu} = \frac{\bar{y}}{\sqrt{\Delta t}} + \frac{1}{2}\hat{\sigma}^2 = \bar{r}+ \frac{1}{2}\hat{\sigma}^2\; .$$ 

Let us digress for a moment to discuss the reliability of $\hat{\mu}$ as an estimate of $\mu$.  Notice that 

$$
\bar{r} 
= \frac{\sum_{i=1}^N \log S_{t_i}-\log S_{t_{i-1}}}{N\Delta t}
$$
$$
=   \frac{\log S_T-\log S_0}{N\Delta t}
$$
$$
 = \frac{\log S_T-\log S_0}{T}\;.
$$ {#eq-volrbar}


Therefore the first component $\bar{r}$ of the estimate of $\mu$ depends only on the total change in $S$ over the time period.  Hence, the reliability of this component cannot depend on how frequently we observe $S$ within the time period $[0,T]$.  The standard deviation of $\bar{r}$ in repeated samples
is the standard deviation of $[\log S_T-\log S_0]/T$, which is $\sigma/\sqrt{T}$.  This is likely to be quite large.  For example, with $\sigma =0.3$ and ten years of data ($T=10$), the standard deviation of $\bar{r}$ is 9.5\%, which means that a 95\% confidence interval will be a band of roughly 38\%.  Given that $\mu$ itself should be of the order of magnitude of 10\%, such a wide confidence interval is useless for all practical purposes.

Fortunately, it is easier to estimate $\sigma$.  We observed in the previous section that the $\hat{\sigma}^2$ defined above can be calculated as
$$
\frac{1}{N-1}\sum_{i=1}^N y_i^2 - \frac{N\bar{y}^2}{N-1}\;.
$$ {#eq-estimator_sig2}

From @eq-volyi of $y_i$ and @eq-volrbar, we have
$$\bar{y} =  \frac{\sqrt{\Delta t}}{T}[\log S_T-\log S_0]\; .$$
Hence, the second term in @eq-estimator_sig2 is
$$ \frac{N}{N-1}\left(\frac{\Delta t}{T^2}\right)[\log S_T-\log S_0]^2\; .$$
If we observe the stock price sufficiently frequently, so that $\Delta t$ is very small, this term will be negligible.  In this circumstance,  $\hat{\sigma}^2$ is approximately

$$
\frac{1}{N-1}\sum_{i=1}^N y_i^2 = \frac{1}{N-1}\sum_{i=1}^N \frac{[\log S_{t_i}-\log S_{t_{i-1}}]^2}{\Delta t}
$$
$$
= \frac{N}{N-1}\times \frac{1}{T}\times \sum_{i=1}^N [\log S_{t_i}-\log S_{t_{i-1}}]^2 \;.
$$ {#eq-estimator_sig2_3}


If we observe $S$ more and more frequently, letting $\Delta t \rightarrow 0$ and $N \rightarrow \infty$, the sum 
$$\sum_{i=1}^N [\log S_{t_i}-\log S_{t_{i-1}}]^2$$
will converge with probability one to $\sigma^2T$, as explained in @sec-s:quadraticvariation.  This implies that $\hat{\sigma}^2$ will converge to $\sigma^2$.  Thus, in theory, we can estimate $\sigma^2$ with any desired degree of precision by simply observing $S$ sufficiently frequently.  This is true no matter how short the overall time period $[0,T]$ may be.  

In practice, this doesn't work out quite so well.  If we observe minute-by-minute data, or we observe each transaction, much of the variation in the price $S$ will be due to bouncing back and forth between the bid price and the ask price.  This is not really what we want to estimate, and this source of variation will be much less important if we look at weekly or even daily data.  So, there are practical limits to how frequently we should observe $S$.  Nevertheless, it is still true that, if $\sigma^2$ were truly constant, we could estimate it with a very high degree of precision.
In fact, we can estimate the volatility of a stock with enough precision to determine that it really isn't constant!  The real problem that we face is to estimate and model a changing volatility.  

## Estimating a Changing Volatility
Without attempting yet to model how the volatility may change, we can say a few things about how we might estimate a changing volatility.  In this and following sections, we will take the observation interval $\Delta t$ to be fixed.  We assume it is small (say, a day or a week) and focus on the estimate @eq-estimator_sig2_3.  Recall from @sec-s:statistics that the reason we are dividing by $N-1$ rather than $N$ is that the sample standard deviation usually underestimates the actual standard deviation, because it uses the sample mean, which will be closer to the points $x_i$ than will be the true mean.  However, @eq-estimator_sig2_3 does not employ the sample mean (it replaces it with zero), so there is no reason to make this correction.  So, we take as our point of departure the estimate
$$\frac{1}{T} \sum_{i=1}^N [\log S_{t_i}-\log S_{t_{i-1}}]^2 = \frac{1}{N}\sum_{i=1}^N y_i^2 \; .$$
An obvious response to the volatility changing over time is simply to avoid using data from the distant past.  Such data is not likely to be informative about the current value of the volatility.  What distant should mean in this context is not entirely clear, but, for example, we might want to use only the last 60 observations.  If we are using daily data, this would mean that at the end of each day we would add that day's observation and drop the observation from 61 days past.  This leads to a somewhat abruptly varying estimate.  For example, a very large movement in the price on a particular day increases the volatility estimate for the next 60 days.  On the 61st day, this observation would drop from the sample, leading to an abrupt drop in the estimate (presuming that there is not an equally large change in $S$ on the 61st day).  This seems unreasonable.  An estimate in which the impact of each observation decays smoothly over time is more attractive.  

We can construct such an estimate as
$$
\hat{\sigma}^2_{i+1} = (1-\lambda) y_{i}^2 + \lambda\hat{\sigma}^2_{i}
$$ {#eq-sig_estimator4}

for any constant $0<\lambda<1$.
Here, $\hat{\sigma}^2_{i+1}$ denotes the estimate of the volatility from date $t_{i}$ to date $t_{i+1}$.  The estimate @eq-sig_estimator4 is a weighted average of the estimate $\hat{\sigma}^2_{i}$ for the previous time period and the most recently observed squared change $y_{i}^2$.  Following the same procedure, the next estimate will be
\begin{align*}
\hat{\sigma}^2_{i+2}& = (1-\lambda) y_{i+1}^2 + \lambda\hat{\sigma}^2_{i+1}\\
&= (1-\lambda) y_{i+1}^2 + \lambda(1-\lambda)  y_{i}^2 + \lambda^2\hat{\sigma}^2_{i}\;.
\end{align*}
Likewise, the estimate at the following date will be
$$\hat{\sigma}^2_{i+3} = (1-\lambda) y_{i+2}^2 +\lambda(1-\lambda) y_{i+1}^2 + \lambda^2(1-\lambda)^2  y_{i}^2 +\lambda^{3}\hat{\sigma}^2_{i}\; .$$
This demonstrates the declining importance of the squared deviation $y_{i}^2$ for future estimates.  At each date, $y_{i}^2$ enters with a weight that is lower by a factor of $\lambda$, compared to the previous date. 
If $\lambda$ is small, the decay in the importance of each squared deviation will be fast.  In fact, @eq-sig_estimator4 shows that, if $\lambda$ is close to zero, the estimate $\hat{\sigma}_{i+1}^2$ is approximately equal to the squared deviation $y_i^2$---previous squared deviations are relatively unimportant.  On the other hand, if $\lambda$ is close to one, the decay will be slow; i.e., the importance of $y_i^2$ for the estimate $\hat{\sigma}^2_{i+2}$ will be nearly the same as for $\hat{\sigma}^2_{i+1}$, and nearly the same for $\hat{\sigma}^2_{i+3}$ as for $\hat{\sigma}^2_{i+2}$, etc.   This will lead to a smooth (slowly varying) volatility estimate.  The slowly varying nature of the estimate in this case is also clear from @eq-sig_estimator4, because it shows that if $\lambda$ is close to one, then $\hat{\sigma}^2_{i+1}$ will be approximately the same as $\hat{\sigma}^2_{i}$.

This method can also be used to estimate covariances, simply by replacing the squared deviations $y_i^2$ by the product of deviations for two different assets.  And, of course, given covariance and variance estimates, we can construct estimates of correlations.  To ensure that an estimated correlation is between $-1$ and $+1$, we will need to use the same $\lambda$ to estimate each of the variances and the covariance.  This is the method used by RiskMetrics.^[See Mina and Xiao [@MX], available online at www.riskmetrics.com]. \index{RiskMetrics}

## GARCH Models {#sec-s:garch}

We are going to adopt a subtle but important change of perspective now.  Instead of considering @eq-sig_estimator4 as simply an estimation procedure, we are going to assume that the actual volatility evolves according to @eq-sig_estimator4, or a generalization thereof.  We are also going to reintroduce the expected change in $\log S$, which we dropped in going from @eq-estimator_sig2 to @eq-estimator_sig2_3.  Specifically, we return to @eq-dlogs, but we operate under the risk-neutral probability, so 
$\mu=r-q$, and we have
$$
\log S(t_{i+1}) - \log S_{t_i} = \left(r-q-\frac{1}{2}\sigma_{i+1}^2\right)\Delta t + \sigma_{i+1} \Delta B\;.
$$ {#eq-dlogs2}

We assume the volatility $\sigma_{i+1}$ between dates $t_i$ and $t_{i+1}$ is given by
$$
\sigma_{i+1}^2 = a + b y_{i}^2 + c \sigma_i^2\;,
$$ {#eq-garch}

for some constants $a > 0$, $b\geq 0$ and $c\geq 0$, with $y_i$ now defined by
$$y_i = \frac{\log S_{t_i}-\log S_{t_{i-1}}-\left(r-q-\frac{1}{2}\sigma_i^2\right)\Delta t}{\sqrt{\Delta t}}\; .$$
From @eq-dlogs2, applied to the period from $t_{i-1}$ to $t_i$, this implies that $y_i$ is normally distributed with mean zero and variance $\sigma_i^2$, and of course $y_{i+1}$ has variance $\sigma_{i+1}^2$, etc.  
Under these assumptions, the random process $\log S$ is called a \index{GARCH process} GARCH(1,1) process.^[GARCH is the acronym for Generalized Autoregressive Conditional Heteroskedastic.  GARCH(1,1) means that there is only one past $y$  (no $y_{i-1}$, $y_{i-2}$, etc.) and one past $\sigma$ (no  $\sigma_{i-1}$, $\sigma_{i-2}$, etc.) in  @eq-garch.  See Bollerslev [@Bollerslev].]  There are many varieties of GARCH processes that have been proposed in the literature, but we will only consider GARCH(1,1), which is the simplest.

We assume $b+c<1$, in which case we can write the variance equation as a generalization of @eq-sig_estimator4.  Namely,
%$$\sigma_{i+1}^2 = (1-\phi)d + \phi\left[(1-\lambda) y_{i}^2 + \lambda \sigma^2_{i}\right]\; ,$$
$$
\sigma_{i+1}^2 = \kappa\theta + (1-\kappa)\left[  (1-\lambda) y_{i}^2 + \lambda\sigma^2_{i}\right]\;,
$$ {#eq-garch10}

where $\lambda=c/(b+c)$, 
%$\phi=b+c$, and $d=a/(1-b-c)$.  
$\kappa = 1-b-c$, and $\theta=a/(1-b-c)$.  Hence, $\sigma_{i+1}^2$ is a weighted average with weights $\kappa$ and $1-\kappa$, of two parts, one being the constant $\theta$ and the other being itself a weighted average of $y_{i}^2$ and $\sigma^2_{i}$.  Whatever the variance might be at time $t_i$, the variance of $y_j$ at any date $t_j$ far into the future, computed without knowing the intervening $y_{i+1}, y_{i+2},\ldots$, will be approximately the constant $\theta$.  The constant $\theta$ is called the unconditional variance, \index{unconditional variance} whereas $\sigma_{i}^2$ is the conditional variance of $y_i$.  \index{conditional variance}

To understand the unconditional variance, it is useful to consider the variance forecasting equation.  Specifically, we can calculate $\\E_{t_i} \left[\sigma_{i+n}^2\right]$, which is the estimate made at date $t_i$ of the variance of $y_{i+n}$; i.e, we estimate the variance without having observed $y_{i+1},\ldots,y_{i+n-1}$.  Note that by definition $\\E_{t_{i}}[y_{i+1}^2]=\sigma_{i+1}^2$, so @eq-garch10 implies
\begin{align*}
\\E_{t_{i}}\left[\sigma_{i+2}^2\right] &= \kappa\theta + (1-\kappa)\left[  (1-\lambda) \\E_{t_{i}}[y_{i+1}^2] + \lambda\sigma^2_{i+1}\right] \\
&= \kappa\theta + (1-\kappa)\sigma^2_{i+1}\; .
\end{align*}
Likewise,
$$\\E_{t_{i+1}}\left[\sigma_{i+3}^2\right] = \kappa\theta + (1-\kappa)\sigma^2_{i+2}\; ,$$
and taking the expectation at date $t_i$ of both sides of this yields
\begin{align*}
\\E_{t_{i}}\left[\sigma_{i+3}^2\right] = \\E_{t_{i}}\left[\\E_{t_{i+1}}\left[\sigma_{i+3}^2\right]\right] &=\kappa\theta + (1-\kappa)\\E_{t_{i}}\left[\sigma_{i+2}^2\right]\\
&=\kappa\theta + (1-\kappa)\left[\kappa\theta + (1-\kappa)\sigma^2_{i+1}\right]\\
&=\kappa\theta[1+(1-\kappa)] + (1-\kappa)^2\sigma^2_{i+1}\;.
\end{align*}
This generalizes to
$$\\E_{t_{i}}\left[\sigma_{i+n}^2\right] = \kappa\theta\left[1+(1-\kappa)+ \cdots (1-\kappa)^{n-2}\right] + (1-\kappa)^{n-1}\sigma^2_{i+1}\; .$$
Thus, there is decay at rate $\kappa$ in the importance of the current volatility $\sigma^2_{i+1}$ for forecasting the future volatility.  Furthermore, as $n\rightarrow \infty$, the geometric series
$$1+(1-\kappa)+ \cdots (1-\kappa)^{n-2}$$
converges to $1/\kappa$, so, as $n \rightarrow \infty$ we obtain
$$\\E_{t_{i}}\left[\sigma_{i+n}^2\right] \rightarrow \theta\; .$$
This means that our best estimate of the conditional variance, at some date far in the future, is approximately the unconditional variance $\theta$.

The most interesting feature of the volatility equation is that large returns (in absolute value) lead to an increase in the variance and hence are likely to be followed by more large returns (whether positive or negative).  This is the phenomenon of volatility clustering, \index{volatility clustering} which is quite observable in actual markets.  This feature also implies that the distribution of returns will be fat tailed  (more technically, leptokurtic).  \index{leptokurtic} This means that the probability of extreme returns is higher than under a normal distribution with the same standard deviation.^[Conversely, the probability of returns very near the mean must also be higher than under a normal distribution with the same standard deviation---a fat-tailed distribution must also have a relatively narrow peak.]  It is well documented that daily and weekly returns in most markets have this fat-tailed property.

We can simulate a path of an asset price that follows a GARCH process and the path of its volatility as follows.  The following python code produces three columns of data (with headings), the first column being time, the second  the asset price, and the third  the volatility.

```{python}
#| code-fold: true
import numpy as np
import pandas as pd

def simulating_garch(S, sigma, r, q, dt, N, theta, kappa, lambd):
    """
    Inputs:
    S = initial stock price
    sigma = initial volatility
    r = risk-free rate
    q = dividend yield
    dt = length of each time period (Delta t)
    N = number of time periods
    theta = theta parameter for GARCH
    kappa = kappa parameter for GARCH
    lambd = lambda parameter for GARCH
    """
    LogS = np.log(S)
    Sqrdt = np.sqrt(dt)
    a = kappa * theta
    b = (1 - kappa) * (1 - lambd)
    c = (1 - kappa) * lambd
    
    time = np.zeros(N + 1)
    stock_price = np.zeros(N + 1)
    volatility = np.zeros(N + 1)
    
    stock_price[0] = S
    volatility[0] = sigma    
    
    for i in range(1, N + 1):
        time[i] = i * dt
        y = sigma * np.random.randn()
        LogS = LogS + (r - q - 0.5 * sigma * sigma) * dt + Sqrdt * y
        S = np.exp(LogS)
        stock_price[i] = S
        sigma = np.sqrt(a + b * y ** 2 + c * sigma ** 2)
        volatility[i] = sigma

    df_garch = pd.DataFrame({'Time': time, 'Stock Price': stock_price, 'Volatility': volatility})
    df_garch.to_csv('garch_simulation.csv', index=False)
    return df_garch

# Example usage:
S = 100       # Initial stock price
sigma = 0.2   # Initial volatility
r = 0.05      # Risk-free rate
q = 0.02      # Dividend yield
dt = 1/252    # Length of each time period (daily)
N = 252       # Number of time periods (one year)
theta = 0.1   # Theta parameter for GARCH
kappa = 0.1   # Kappa parameter for GARCH
lambd = 0.9   # Lambda parameter for GARCH

df_garch = simulating_garch(S, sigma, r, q, dt, N, theta, kappa, lambd)
print(df_garch)
```

To price European options, we need to compute the usual probabilities 
$\text{prob}^S(S_T>K)$ and $\text{prob}^R(S_T >K)$.
Heston and Nandi [@HN] provide a fast method for computing these probabilities in a GARCH (1,1) model.^[Actually, a slightly more general model is considered in [@HN], in which large negative returns lead to a greater increase in volatility than do large positive returns.  This accommodates the empirically observed negative correlation between stock returns and volatility.]  Rather than developing this approach, we will show in @sec-c:introcomputation how to apply Monte-Carlo methods.

## Hedging and Market Completeness

The GARCH model is inherently a discrete-time model.  If returns have a GARCH structure at one frequency (e.g., monthly), they will not have a GARCH structure at a different frequency (e.g., weekly).  Hence, the return period (monthly, weekly, \ldots) is part of the specification of the model.  One interpretation of the model is that the dates $t_i$ at which the variance changes are the only dates at which investors can trade.  Under this interpretation, it is impossible to perfectly hedge an option: the gross return $S_{t_i}/S_{t_{i-1}}$ over the interval $(t_{i-1},t_i)$ is lognormally distributed, so no portfolio of the stock and riskless asset formed at $t_{i-1}$ and held over the interval $(t_{i-1},t_i)$ can perfectly replicate the return of an option over the interval.  As discussed in @sec-s:incomplete, we call a market in which some derivatives cannot be perfectly hedged an incomplete market. \index{incomplete market}  Thus, the GARCH model is an example of an incomplete market, if investors can only trade at the frequency at which returns have a GARCH structure.  However, it is unreasonable to assume that investors can only trade weekly or monthly or even daily.

Another interpretation of the GARCH model is that investors can trade continuously and the asset has a constant volatility within each period $(t_{i-1},t_i)$.  Under this interpretation, the market is complete and options can be delta-hedged.  The completeness is a result of the fact that the change $\sigma_{i+1}-\sigma_i$ in the volatility  at date $t_i$ (recall that $\sigma_i$ is the volatility over the period $(t_{i-1},t_i)$ and $\sigma_{i+1}$ is the volatility over the period $(t_{i},t_{i+1})$) depends only on $\log S_{t_i}$.  Thus, the only random factor in the model that needs to be hedged is, as usual, the underlying asset price.  However, this interpretation of the model is also a bit strange.  Suppose for example that monthly returns are assumed to have a GARCH structure.  Then the model states that the volatility in February will be higher if there is an unusually large return (in absolute value) in January.  Suppose there is an unusually large return in the first half of January.  Then, intuitively, one would expect the change in the volatility to occur in the second half of January rather than being delayed until February.  However, the model specifies that the volatility is constant during each month, hence constant during January in this example.



## {.unnumbered}

::: {#exr-e_mixture}
  The purpose of this exercise is to generate a fat-tailed distribution from a model that is simpler than the GARCH and stochastic volatility models but has somewhat the same flavor.  The distribution will be a mixture of normals. Create a python program in which the user can input $S$, $r$, $q$, $T$, $\sigma_1$ and $\sigma_2$.  Use these inputs to produce a column of 500 simulated $\log S_T$.  In each simulation, define $\log S_T$ as
$$\log S_T = \log S_0 + \left(r-q-\frac{1}{2}\sigma^2\right)T + \sigma \sqrt{T}z\;,$$
where $z$ is a standard normal,
$\sigma = x\sigma_1 + (1-x)\sigma_2$,
and $x$ is a random variable that equals zero or one with equal probabilities.  

Calculate the mean and standard deviation of the $\log S_T$ and calculate the fraction that lie more than two standard deviations below the mean.  If the $\log S_T$ all came from a normal distribution with the same variance, then this fraction should equal $\mathrm{N}(-2) =$ 2.275\%.  If the fraction is higher, then the distribution is fat tailed.  (Of course, the actual fraction would differ from 2.275\% in any particular case due to the randomness of the simulation, even if all of the $\log S_T$ came from a normal distribution with the same variance).
:::

::: {#exr-e_GARCH1}
  Create a python program prompting the user to input the same inputs as in the `simulating_garch` function except for the initial volatility and $\theta$.  Simulate 500 paths of a GARCH process and output $\log S_T$ for each simulation (you don't need to output the entire paths as in the `simulating_garch` function).  Take the initial volatility to be 0.3 and $\theta = 0.09$.  Determine whether the distribution is fat-tailed by computing the fraction of the $\log S_T$ that lie two or more standard deviations below the mean, as in the previous exercise.  For what values of $\kappa$ and $\lambda$ does the distribution appear to be especially fat-tailed? 
:::
::: Exercise
 Repeat @exr-e_GARCH1 for the Heston stochastic volatility model, describing the values of $\kappa$,  $\gamma$ and $\rho$ that appear to generate especially fat-tailed distributions.

:::
