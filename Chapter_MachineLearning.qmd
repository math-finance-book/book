{{< include macros.qmd >}}

# Machine Learning {#sec-c:machinelearning} 



The world of finance has witnessed a surge in the application of machine learning (ML) techniques to solve complex problems. Among the most prominent areas of application is derivative security pricing and hedging. Derivative securities, such as options, futures, and swaps, often involve sophisticated pricing models and hedging strategies to manage risk effectively. Traditional methods, such as the Black-Scholes model and its variants, rely on specific assumptions that may not hold in all market conditions. Machine learning offers a data-driven approach to enhance accuracy and flexibility in these tasks. In this chapter, we explore the application of machine learning techniques to derivative pricing and hedging, using Python programs to illustrate key concepts and numerical results.

## Theoretical Background

### Overview of Derivative Pricing

Derivative pricing involves determining the fair value of a financial instrument derived from an underlying asset. Popular models include:

1. Black-Scholes Model: Assumes constant volatility and continuous trading.
2. Binomial Tree Model: Provides a discrete-time framework for option pricing.
3. Monte Carlo Simulation: Estimates prices by simulating paths of the underlying asset.

### Challenges in Traditional Models

1. Non-constant Volatility: Market volatility often varies over time.
2. Non-stationary Data: Real-world data often exhibit patterns that change over time.
3. Model Assumptions: Many traditional models rely on unrealistic assumptions, such as frictionless markets and log-normal price distributions.

### Role of Machine Learning

Machine learning offers tools to address these challenges by leveraging historical data and identifying complex patterns that traditional models may miss. Key advantages include:

1. Flexibility: Ability to model non-linear relationships.
2. Adaptability: Can handle non-stationary and high-dimensional data.
3. Improved Hedging: Enhanced prediction of risk measures, leading to better hedging strategies.

## Machine Learning Techniques for Derivative Pricing

### Supervised Learning for Option Pricing

Supervised learning can be used to predict option prices based on features such as the underlying asset price, volatility, time to maturity, and interest rates. Popular algorithms include:

1. Linear Regression: For simple pricing models.
2. Decision Trees: To capture non-linear relationships.
3. Neural Networks: For high-dimensional and non-linear data.

#### Python Example: Neural Network for Option Pricing

This example illustrates how to use a Neural Network to estimate option prices using scikit-learn. We generate synthetic data using the Black-Scholes model and train a Multi-Layer Perceptron (MLP) Regressor to predict option prices.

**Step 1: Importing Required Libraries**

We start by importing necessary Python libraries:

- `numpy` for numerical operations
- `pandas` for handling tabular data
- `scikit-learn` (imported as `sklearn`) for machine learning
- `scipy.stats` for computing statistical functions (used in Black-Scholes)

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error
from scipy.stats import norm
```

---

**Step 2: Generating Synthetic Data**

To simulate real-world financial data, we generate 10,000 synthetic samples using random values for:

- Underlying Price (S): Randomly chosen between $50 and $150
- Volatility (Ïƒ): Randomly chosen between 0.1 and 0.5
- Time to Maturity (T): Randomly chosen between 0.01 and 1 year
- Interest Rate (r): Randomly chosen between 1% and 5%
- Strike Price (K): Randomly chosen between $50 and $150

```python
np.random.seed(42)  # Ensuring reproducibility
num_samples = 10000
underlying_price = np.random.uniform(50, 150, num_samples)
volatility = np.random.uniform(0.1, 0.5, num_samples)
time_to_maturity = np.random.uniform(0.01, 1, num_samples)
interest_rate = np.random.uniform(0.01, 0.05, num_samples)
strike_price = np.random.uniform(50, 150, num_samples)
```

---

**Step 3: Computing Option Prices using Black-Scholes Model**

The Black-Scholes formula estimates the theoretical price of a European call option:

$$
d_1 = \frac{\ln(\frac{S}{K}) + (r + \frac{1}{2} \sigma^2)T}{\sigma \sqrt{T}}
$$

$$
d_2 = d_1 - \sigma \sqrt{T}
$$

$$
C = S N(d_1) - K e^{-rT} N(d_2)
$$

where:
- $N(d)$ is the cumulative normal distribution function.
- $C$ is the option price.

We use this model to generate synthetic option prices:

```python
def black_scholes_price(S, K, T, r, sigma):
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = d1 - sigma * np.sqrt(T)
    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)

option_price = black_scholes_price(underlying_price, strike_price, time_to_maturity, interest_rate, volatility)
```

---

**Step 4: Preparing the Dataset**

We create a pandas DataFrame with five input features and the computed option price as the target:

```python
data = pd.DataFrame({
    'Underlying Price': underlying_price,
    'Volatility': volatility,
    'Time to Maturity': time_to_maturity,
    'Interest Rate': interest_rate,
    'Strike Price': strike_price,
    'Option Price': option_price
})
```

We split the dataset into training (80%) and testing (20%) sets:

```python
X = data[['Underlying Price', 'Volatility', 'Time to Maturity', 'Interest Rate', 'Strike Price']]
y = data['Option Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

---

**Step 5: Training a Neural Network Model**

We train a Multi-Layer Perceptron (MLP) Regressor, a type of neural network that predicts option prices:

- Hidden layers: Two layers with 50 neurons each.
- Max iterations: 500 (controls training duration).
- Random state: 42 (ensures reproducibility).

```python
model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42)
model.fit(X_train, y_train)
```

---

**Step 6: Model Evaluation**

We use Mean Squared Error (MSE) to evaluate how well the neural network predicts option prices:

```python
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")
```

A lower MSE indicates that the model is making better predictions.

In summary, 

- We used the Black-Scholes model to generate synthetic option prices.
- We trained a Neural Network using scikit-learn to estimate option prices.
- The trained model can now be used for predicting option prices based on market conditions.


The complete code is presented below.


```{python}
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_squared_error

# Generate synthetic data for option pricing
np.random.seed(42)
num_samples = 10000
underlying_price = np.random.uniform(50, 150, num_samples)
volatility = np.random.uniform(0.1, 0.5, num_samples)
time_to_maturity = np.random.uniform(0.01, 1, num_samples)
interest_rate = np.random.uniform(0.01, 0.05, num_samples)

# Black-Scholes formula for synthetic option prices
def black_scholes_price(S, K, T, r, sigma):
    from scipy.stats import norm
    d1 = (np.log(S / K) + (r + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))
    d2 = d1 - sigma * np.sqrt(T)
    return S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)

strike_price = np.random.uniform(50, 150, num_samples)
option_price = black_scholes_price(underlying_price, strike_price, time_to_maturity, interest_rate, volatility)

# Prepare the dataset
data = pd.DataFrame({
    'Underlying Price': underlying_price,
    'Volatility': volatility,
    'Time to Maturity': time_to_maturity,
    'Interest Rate': interest_rate,
    'Strike Price': strike_price,
    'Option Price': option_price
})

X = data[['Underlying Price', 'Volatility', 'Time to Maturity', 'Interest Rate', 'Strike Price']]
y = data['Option Price']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a neural network model
model = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=500, random_state=42)
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse:.4f}")
```

This approach is useful for quantitative finance applications, where machine learning enhances option pricing models.


### Estimating Implied Volatility Surface Using Machine Learning

Implied volatility (IV) is a critical measure in option pricing that reflects market expectations of future volatility. The IV surface varies with strike prices and maturities. Machine learning techniques can be applied to estimate this surface efficiently from option price data.

#### Python Example: Neural Network for IV Surface Estimation

This erxample illustrates how to use a Random Forest Regressor to estimate Implied Volatility (IV) from option pricing data. The implied volatility is a key metric in finance that represents the market's expectations of future volatility.

We assume that the actual volatility used in the Black-Scholes model is a proxy for implied volatility in this demonstration.

---

**Step 1: Importing Required Libraries**

We start by importing the necessary Python libraries:

```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
```

- `RandomForestRegressor`: A machine learning model that builds an ensemble of decision trees for regression tasks.
- `train_test_split`: Splits the dataset into training and testing sets.
- `mean_squared_error`: Measures the error in the model's predictions.

---

**Step 2: Preparing the Dataset for IV Estimation**

What is Implied Volatility (IV)?
- Implied volatility is not directly observable in the market.
- It is usually derived from option prices using models like Black-Scholes.
- Here, we approximate IV using the volatility values from our dataset.

We create a copy of the dataset and introduce a new column for Implied Volatility:

```python
iv_data = data.copy()
iv_data['Implied Volatility'] = volatility  # Assume volatility as IV for demonstration purposes
```

We select the following features (`X_iv`) to predict IV:
- Underlying Price
- Strike Price
- Time to Maturity
- Option Price

The target variable (`y_iv`) is Implied Volatility:

```python
X_iv = iv_data[['Underlying Price', 'Strike Price', 'Time to Maturity', 'Option Price']]
y_iv = iv_data['Implied Volatility']
```

We split the data into 80% training and 20% testing:

```python
X_iv_train, X_iv_test, y_iv_train, y_iv_test = train_test_split(X_iv, y_iv, test_size=0.2, random_state=42)
```

---

**Step 3: Training a Random Forest Model**

A Random Forest Regressor is an ensemble model that builds multiple decision trees and averages their predictions for robust and accurate results.

We train a Random Forest model with:
- 100 estimators (trees)
- Fixed random state (42) for reproducibility

```python
iv_model = RandomForestRegressor(n_estimators=100, random_state=42)
iv_model.fit(X_iv_train, y_iv_train)
```

---

**Step 4: Predicting and Evaluating the Model**

After training the model, we predict Implied Volatility (IV) for the test set:

```python
y_iv_pred = iv_model.predict(X_iv_test)
```

To evaluate model performance, we calculate the Mean Squared Error (MSE):

```python
mse_iv = mean_squared_error(y_iv_test, y_iv_pred)
print(f"IV Surface Estimation Mean Squared Error: {mse_iv:.4f}")
```

---

**Step 5: Interpretation of Results**

- The Mean Squared Error (MSE) measures how well the model predicts implied volatility.
- A lower MSE means better accuracy.
- The trained Random Forest model can now estimate implied volatility based on option market data.

---

In summary, 

- We trained a Random Forest Regressor to estimate Implied Volatility (IV).
- The model uses option price, underlying price, strike price, and time to maturity as features.
- This approach can be used in quantitative finance to model IV surfaces dynamically.


The complete code and output is as follows.

```{python}
from sklearn.ensemble import RandomForestRegressor

# Prepare features for IV estimation
iv_data = data.copy()
iv_data['Implied Volatility'] = volatility  # Assume volatility as IV for demonstration purposes
X_iv = iv_data[['Underlying Price', 'Strike Price', 'Time to Maturity', 'Option Price']]
y_iv = iv_data['Implied Volatility']
X_iv_train, X_iv_test, y_iv_train, y_iv_test = train_test_split(X_iv, y_iv, test_size=0.2, random_state=42)

# Train a Random Forest model for IV estimation
iv_model = RandomForestRegressor(n_estimators=100, random_state=42)
iv_model.fit(X_iv_train, y_iv_train)

# Predict and evaluate
y_iv_pred = iv_model.predict(X_iv_test)
mse_iv = mean_squared_error(y_iv_test, y_iv_pred)
print(f"IV Surface Estimation Mean Squared Error: {mse_iv:.4f}")
```

### Estimating Heston Model Coefficients Using Machine Learning

The Heston model is a popular stochastic volatility model with parameters that can be challenging to estimate from market data. Machine learning provides a data-driven approach to estimate these coefficients.

**Python Example: Gradient Boosting for Heston Model Estimation**

This example illustrates how to estimate Heston model parameters using Gradient Boosting Regression. The Heston model is a widely used stochastic volatility model in financial mathematics to describe the evolution of asset prices.

We generate synthetic data for Heston model parameters and train a Gradient Boosting Regressor to estimate option prices based on these parameters.

---

**Step 1: Importing Required Libraries**

```python
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd
```

- `GradientBoostingRegressor`: A machine learning algorithm that builds an ensemble of weak prediction models in a stage-wise fashion.
- `train_test_split`: Splits the dataset into training and testing sets.
- `mean_squared_error`: Measures the model's prediction accuracy.
- `numpy`: Used to generate synthetic numerical data.
- `pandas`: Used to store and manipulate data.

---

**Step 2: Generating Synthetic Data for the Heston Model**

The Heston model describes asset price evolution with a stochastic variance component:

$$
dS_t = \mu S_t dt + \sqrt{v_t} S_t dW_t^S
$$

$$
dv_t = \kappa (\theta - v_t) dt + \sigma_v \sqrt{v_t} dW_t^v
$$

where:
- $ v_0 $ = Initial variance
- $ \kappa $ = Mean reversion speed of variance
- $ \theta $ = Long-term variance level
- $ \sigma_v $ = Volatility of variance
- $ \rho $ = Correlation between asset price and variance

We generate 5,000 synthetic samples for these parameters:

```python
np.random.seed(42)  # Ensuring reproducibility
num_samples_heston = 5000
v0 = np.random.uniform(0.1, 0.3, num_samples_heston)  # Initial variance
kappa = np.random.uniform(1, 5, num_samples_heston)  # Mean reversion speed
theta = np.random.uniform(0.1, 0.3, num_samples_heston)  # Long-term variance
sigma_v = np.random.uniform(0.2, 0.5, num_samples_heston)  # Volatility of variance
rho = np.random.uniform(-1, 1, num_samples_heston)  # Correlation coefficient
```

---

**Step 3: Creating a Synthetic Dataset**

We store the generated parameters in a pandas DataFrame:

```python
heston_data = pd.DataFrame({
    'v0': v0,
    'kappa': kappa,
    'theta': theta,
    'sigma_v': sigma_v,
    'rho': rho
})
```

**Generating Synthetic Option Prices**

For demonstration, we assume that option prices are related to Heston parameters using a simple linear function:

$$
\text{Option Price} = v_0 \cdot \kappa + \theta + \sigma_v \cdot | \rho |
$$

This is a simplified representation and does not correspond to the exact Heston formula.

```python
heston_data['Option Price'] = (
    heston_data['v0'] * heston_data['kappa'] + heston_data['theta'] +
    heston_data['sigma_v'] * np.abs(heston_data['rho'])
)
```

---

**Step 4: Splitting the Dataset**

We define:
- Input Features (`X_heston`): The five Heston model parameters.
- Target Variable (`y_heston`): The synthetic option price.

We split the dataset into 80% training and 20% testing:

```python
X_heston = heston_data[['v0', 'kappa', 'theta', 'sigma_v', 'rho']]
y_heston = heston_data['Option Price']
X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(X_heston, y_heston, test_size=0.2, random_state=42)
```

---

**Step 5: Training a Gradient Boosting Model**

What is Gradient Boosting?
- Gradient Boosting Regressor (GBR) is a powerful machine learning technique that builds sequential decision trees to minimize prediction error.
- It iteratively improves predictions by learning from previous errors.

We train a GBR model with:
- 100 trees (n_estimators=100)
- Random state 42 (for reproducibility)

```python
heston_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
heston_model.fit(X_h_train, y_h_train)
```

---

**Step 6: Model Evaluation**

Once trained, the model predicts option prices for the test set:

```python
heston_pred = heston_model.predict(X_h_test)
```

** Measuring Accuracy**

We evaluate the model using Mean Squared Error (MSE):

```python
heston_mse = mean_squared_error(y_h_test, heston_pred)
print(f"Heston Coefficients Estimation Mean Squared Error: {heston_mse:.4f}")
```

A lower MSE indicates better prediction accuracy.

---
In summary,

- We generated synthetic Heston model parameters and related option prices.
- We trained a Gradient Boosting Regressor to estimate option prices based on Heston parameters.
- The trained model can be used for Heston parameter calibration in real-world financial applications.

This approach can help quants, traders, and risk analysts estimate option prices based on stochastic volatility models.

---

The follwoing is the complete code with the output

```{python}
from sklearn.ensemble import GradientBoostingRegressor

# Generate synthetic data for Heston model estimation
np.random.seed(42)
num_samples_heston = 5000
v0 = np.random.uniform(0.1, 0.3, num_samples_heston)  # Initial variance
kappa = np.random.uniform(1, 5, num_samples_heston)  # Mean reversion speed
theta = np.random.uniform(0.1, 0.3, num_samples_heston)  # Long-term variance
sigma_v = np.random.uniform(0.2, 0.5, num_samples_heston)  # Volatility of variance
rho = np.random.uniform(-1, 1, num_samples_heston)  # Correlation coefficient

# Simulate Heston parameters
heston_data = pd.DataFrame({
    'v0': v0,
    'kappa': kappa,
    'theta': theta,
    'sigma_v': sigma_v,
    'rho': rho
})

# Assume synthetic market data related to these parameters (e.g., option prices)
heston_data['Option Price'] = (
    heston_data['v0'] * heston_data['kappa'] + heston_data['theta'] +
    heston_data['sigma_v'] * np.abs(heston_data['rho'])
)

# Split data
X_heston = heston_data[['v0', 'kappa', 'theta', 'sigma_v', 'rho']]
y_heston = heston_data['Option Price']
X_h_train, X_h_test, y_h_train, y_h_test = train_test_split(X_heston, y_heston, test_size=0.2, random_state=42)

# Train Gradient Boosting model
heston_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
heston_model.fit(X_h_train, y_h_train)

# Evaluate
heston_pred = heston_model.predict(X_h_test)
heston_mse = mean_squared_error(y_h_test, heston_pred)
print(f"Heston Coefficients Estimation Mean Squared Error: {heston_mse:.4f}")
```

## Reinforcement Learning for Hedging

Reinforcement learning (RL) is well-suited for dynamic hedging, where the goal is to minimize risk by adjusting the hedge portfolio over time.

**Python Example: Using RL to Price Asian Options.**

This project consists of three major steps:
1. Simulating stock prices using a Gamma distribution instead of the traditional Geometric Brownian Motion (GBM).
2. Pricing Asian call options using Monte Carlo simulation.
3. Using Reinforcement Learning (RL) to learn the pricing function and evaluate its post-training accuracy.

---

**Step 1: Simulating Gamma-Distributed Stock Prices**

We assume that the underlying stock price follows a Gamma process instead of GBM:
$$
S_t = S_0 + \sum_{i=1}^{t} X_i
$$
where $X_i$ follows a Gamma distribution with shape $k$ and scale $\theta$.

---

**Step 2: Monte Carlo Pricing of Asian Options**

Asian call options depend on the average price over the option's life:
$$
C = e^{-rT} \max \left( \frac{1}{N} \sum_{i=1}^{N} S_{t_i} - K, 0 \right)
$$
where:
- $S_{t_i}$ are simulated Gamma-distributed stock prices.
- $N$ is the number of observation points.
- $K$ is the strike price.
- $r$ is the risk-free rate.
- $T$ is the time to maturity.

---

**Step 3: Reinforcement Learning for Option Pricing**

- We train an RL agent to predict the Asian call price given:
  - Gamma distribution parameters (shape $k$, scale $\theta$)
  - Time to maturity $T$
  - Risk-free rate $r$
  - Strike price $K$
- After training, we evaluate the pricing error in a post-training test set.


Explanation of the Code

**Step 1: Simulating Gamma-Distributed Stock Prices**

- We generate stock prices using a Gamma distribution.
- Each price follows:
  $$
  S_t = S_0 + \sum_{i=1}^{t} X_i
  $$
  where $ X_i \sim \Gamma(k, \theta) $.

**Step 2: Monte Carlo Pricing of Asian Options**

- We generate 10000 simulated paths for stock prices.
- The Asian option price is computed as:
  $$
  C = e^{-rT} \max \left( \frac{1}{N} \sum_{i=1}^{N} S_{t_i} - K, 0 \right)
  $$
- Training data consists of different combinations of:
  - Gamma parameters: $k, \theta$
  - Option strike price $ K $
  - Risk-free rate $ r $
  - Time to maturity $ T $

**Step 3: Training the RL Model**

- We train a Neural Network to approximate the pricing function.
- The network takes 5 inputs (shape, scale, K, r, T) and outputs the Asian call price.

**Step 4: Evaluating Post-Training Accuracy**

- We generate new test data and compare RL model predictions vs. Monte Carlo prices.
- We compute and plot the pricing error distribution.


The complete code is as follows.

```{python}
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque
import matplotlib.pyplot as plt
from scipy.stats import gamma

# Monte Carlo Simulation for Asian Option Pricing
def simulate_gamma_stock_prices(S0, shape, scale, T, N):
    dt = T / N  # Time step
    gamma_draws = gamma.rvs(a=shape, scale=scale, size=(N,))
    stock_prices = S0 + np.cumsum(gamma_draws)  # Gamma distributed stock prices
    return stock_prices

# Monte Carlo Asian Call Option Pricing
def monte_carlo_asian_call(S0, shape, scale, K, r, T, N, num_simulations=10000):
    option_payoffs = []
    for _ in range(num_simulations):
        stock_prices = simulate_gamma_stock_prices(S0, shape, scale, T, N)
        avg_price = np.mean(stock_prices)
        payoff = max(avg_price - K, 0)
        option_payoffs.append(payoff)

    option_price = np.exp(-r * T) * np.mean(option_payoffs)
    print("Option price...", option_price)
    return option_price

# Generate Training Data
def generate_training_data(num_samples=5000):
    data = []
    for _ in range(num_samples):
        shape = np.random.uniform(1, 5)  # Gamma shape parameter
        scale = np.random.uniform(0.5, 2)  # Gamma scale parameter
        K = np.random.uniform(80, 120)  # Strike price
        r = np.random.uniform(0.01, 0.05)  # Risk-free rate
        T = np.random.uniform(0.1, 2)  # Time to maturity
        S0 = 100  # Initial stock price
        N = 50  # Number of time steps

        option_price = monte_carlo_asian_call(S0, shape, scale, K, r, T, N)
        data.append([shape, scale, K, r, T, option_price])

    return np.array(data)

# Define the RL Model (Neural Network)
class RL_PricingModel(nn.Module):
    def __init__(self, input_dim):
        super(RL_PricingModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Train the RL Model
def train_rl_model(train_data, num_epochs=1000, batch_size=64):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = RL_PricingModel(input_dim=5).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    loss_function = nn.MSELoss()

    # Convert training data to tensors
    X_train = torch.FloatTensor(train_data[:, :-1]).to(device)
    y_train = torch.FloatTensor(train_data[:, -1]).unsqueeze(1).to(device)

    for epoch in range(num_epochs):
        optimizer.zero_grad()
        y_pred = model(X_train)
        loss = loss_function(y_pred, y_train)
        loss.backward()
        optimizer.step()

        if epoch % 100 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.4f}")

    return model

# Generate Post-Training Test Data
def generate_test_data(num_samples=1000):
    return generate_training_data(num_samples)

# Evaluate the Model on Test Data
def evaluate_model(model, test_data):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X_test = torch.FloatTensor(test_data[:, :-1]).to(device)
    y_test = test_data[:, -1]
    
    model.eval()
    with torch.no_grad():
        y_pred = model(X_test).cpu().numpy().flatten()

    pricing_error = np.abs(y_pred - y_test)
    mean_error = np.mean(pricing_error)
    
    print(f"Mean Pricing Error: {mean_error:.4f}")
    
    # Plot the pricing error distribution
    plt.hist(pricing_error, bins=30, alpha=0.7, color='blue')
    plt.xlabel("Pricing Error")
    plt.ylabel("Frequency")
    plt.title("Pricing Error Distribution (Post-Training)")
    plt.show()

# Main Execution
if __name__ == "__main__":
    print("Generating training data...")
    train_data = generate_training_data()

    print("Training RL model...")
    model = train_rl_model(train_data)

    print("Generating post-training test data...")
    test_data = generate_test_data()

    print("Evaluating RL model...")
    evaluate_model(model, test_data)


```

## Conclusion

Machine learning provides powerful tools for derivative pricing and hedging, addressing many limitations of traditional methods. By leveraging techniques such as neural networks and reinforcement learning, practitioners can enhance accuracy, adaptability, and risk management. The Python examples presented here illustrate practical implementations of these methods, serving as a foundation for further exploration in this exciting field.



```{python}
#| eval: true
#| echo: false

import plotly
from IPython.display import display, HTML

plotly.offline.init_notebook_mode(connected=True)
display(
    HTML(
        '<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>'
    )
)
```




## Latex 

Trying to get latex to work. Still works

```{python}
#| eval: true
#| echo: false

import plotly
from IPython.display import display, HTML

plotly.offline.init_notebook_mode(connected=True)
display(HTML(
    '<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>'
))
```


```{python}
#| output: true


import numpy as np
import plotly.graph_objects as go

mu = 1
sigma = 1
t = 1

# Brownian path
n = 1000   
dt = t/n
dB = np.random.normal(scale = np.sqrt(dt), size=n)
B = np.zeros(n+1)
B[1:] = np.cumsum(dB)

# Brownian path with discrete steps
n_discrete = 10
Delta_t = t/n_discrete
B_discrete = B[::int(n/n_discrete)]
DeltaB = np.diff(B_discrete)

# X in discrete-time 
X1 = np.ones(n_discrete+1)
for i in range(1, n_discrete+1):
    X1[i] = X1[i-1] + mu * X1[i-1] * Delta_t + sigma * X1[i-1] * DeltaB[i-1]

# Continuous-time
X2 = np.exp((mu - 0.5 * sigma**2) * np.arange(0, t+dt, dt) + sigma * B)

fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=np.arange(0, t+Delta_t, Delta_t), 
        y=X1, 
        mode='lines+markers', 
        line={'shape': 'spline', 'smoothing': 0},
        name=r'$X_{ti}-X_{ti-1}$',
        hovertemplate='t = %{x:.2f}<br>B = %{y:.2f}<extra></extra>'  # 
    )
)

fig.add_trace(
    go.Scatter(
        x=np.arange(0, t+dt, dt), 
        y=X2, 
        mode='lines', 
        name=r'$dX_t$',
        hovertemplate='t = %{x:.2f}<br>B = %{y:.2f}<extra></extra>'  # 
    )
)
fig.update_layout(
    showlegend=True,
    xaxis_title='Time',
    yaxis_title='',
    template='plotly_white',
    height=300,
    width=450,
    legend=dict(
        x=0.1,
        y=1,
        xanchor="left",
        yanchor="top",
    )
)

fig.show()
```


![](under_construction.jpg)