# Estimating and Modelling Volatility {#sec-c_stochasticvolatility}

Thus far, we have assumed that the volatility of the underlying asset is constant or varying in a non-random way during the lifetime of the derivative.  In this chapter we will look at models that relax this assumption and allow the volatility to change randomly.  This is very important, because there is plenty of evidence that volatilities do change over time in a random way.  

In the first three sections, we will consider the problem of estimating the volatility.  The discussion of estimation methods leads naturally into the discussion of modelling a changing volatility.  

## Statistics Review {#sec-s_statistics}

We begin with a brief review of basic statistics.
Given a random sample $\{x_1,\ldots,x_N\}$ of size $N$ from a population with mean $\mu$ and variance $\sigma^2$, the best estimate of $\mu$ is of course the sample mean 
$$\bar{x} = \frac{1}{N}\sum_{i=1}^{N}x_i\; .$$
The variance is the expected value of $(x-\mu)^2$, so an obvious estimate of the variance is the sample average of $(x_i-\mu)^2$, replacing $\mu$ with its estimate $\bar{x}$.  This would be
$$\frac{1}{N}\sum_{i=1}^{N} (x_i-\bar{x})^2$$
However, because $\bar{x}$ is computed from the $x_i$, the $x_i$ will deviate less on average from $\bar{x}$ than they do from the true mean $\mu$.  Hence the estimate proposed above will on average be less than $\sigma^2$.  To eliminate this bias, it suffices just to scale the estimate up by a factor of $N/(N-1)$.  This leads to the estimate
$$s^2=\frac{1}{N-1}\sum_{i=1}^{N} (x_i-\bar{x})^2\; ,$$
and the best estimate of $\sigma$ is the square root
$$s=\sqrt{\frac{1}{N-1}\sum_{i=1}^{N} (x_i-\bar{x})^2}\; .$$
To calculate $s^2$, notice that 
$$
\sum_{i=1}^{N} (x_i-\bar{x})^2 = \sum_{i=1}^{N} (x_i^2-2x_i\bar{x}+\bar{x}^2)
$$
$$
=\sum_{i=1}^{N} x_i^2 -2\bar{x}\sum_{i=1}^{N} x_i + \sum_{i=1}^N \bar{x}^2
$$
$$
=\sum_{i=1}^{N} x_i^2 -2\bar{x}(N\bar{x})+N\bar{x}^2
$$
$$
=\sum_{i=1}^{N} x_i^2 -N\bar{x}^2\;.
$$
Therefore
$$s=\sqrt{\frac{1}{N-1}\left(\sum_{i=1}^{N} x_i^2-N\bar{x}^2\right)}\; .$$

It is important to know how much variation there would be in $\bar{x}$ if one had access to multiple random samples.  More variation means that an $\bar{x}$ computed from a single sample will be a less reliable estimate of $\mu$.  The variance of $\bar{x}$ in repeated samples is $\sigma^2/N$,^[The variance of $\bar{x] = (1/N)(x_1 + \cdots + x_N)$ is, by independence of the $x_i$, equal to $(1/N)^2(\mathrm{var}{x_1} + \cdots + \mathrm{var}{x_N})$, and, because the $x_i$ all have the same variance $\sigma^2$, this is equal to $(1/N)^2 \times N\sigma^2 = \sigma^2/N$.} and our best estimate of this variance is $s^2/N$.  The standard deviation of $\bar{x}$ in repeated samples, which is called the standard error of \index{standard error} $\bar{x}$, is $\sigma/\sqrt{N}$, and we estimate this by $s/\sqrt{N}$, which equals
$$\sqrt{\frac{1}{N(N-1)}\left(\sum_{i=1}^{N} x_i^2-N\bar{x}^2\right)}\; .$$
If the population from which $x$ is sampled has a normal distribution, then a 95\% confidence interval for $\mu$ will be $\bar{x}$ plus or minus 1.96 standard errors.  Even if $x$ does not have a normal distribution, by the Central Limit Theorem, $\bar{x}/\sqrt{N}$ will be approximately normally distributed if the sample size $N$ is large enough, and plus or minus 1.96 standard errors will still be approximately a 95\% confidence interval for~$\mu$. \index{confidence interval}

## Estimating a Constant Volatility and Mean {#sec-s_estimatingvolatility}

Consider an asset price that is a geometric Brownian motion under the actual probability measure:
$$\frac{\mathrm{d} S}{S} = \mu\,\mathrm{d} t + \sigma\,\mathrm{d} B\; ,$$
where $\mu$ and $\sigma$ are unknown constants and $B$ is a Brownian motion.  We can as usual write this in log form as
$$\mathrm{d}\log S = \left(\mu-\frac{1}{2}\sigma^2\right)\,\mathrm{d} t + \sigma\,\mathrm{d} B\; .$$
Over a discrete time period of length $\Delta t$, this implies
$$
\Delta \log S = \left(\mu-\frac{1}{2}\sigma^2\right)\Delta t + \sigma \Delta B\;.
$$ {#eq-dlogs}

Suppose we have observed the asset price $S$ at dates $0=t_0<t_1<\cdots< t_N=T$, where $t_i-t_{i-1}=\Delta t$.  If the asset pays dividends, we will take~$S$ to be the value of the portfolio in which the dividends are reinvested in new shares.  Thus, in general, $S(t_i)/S(t_{i-1})$ denotes the gross return (one plus the rate of return) between dates $t_{i-1}$ and $t_i$.  This return is measured on a non-compounded and non-annualized basis.  The annualized continuously-compounded rate of return is the rate $r_i$ defined by 
$$\frac{S(t_i)}{S(t_{i-1})} = \mathrm{e}^{r_i\Delta t}\; .$$
This implies that
$$
r_i = \frac{\log S(t_i)-\log S(t_{i-1})}{\Delta t} = \mu-\frac{1}{2}\sigma^2 + \sigma \frac{B(t_i)-B(t_{i-1})}{\Delta t}\;.
$$ {#eq-contcompreturn}

Because $B(t_i)-B(t_{i-1})$ is normally distributed with mean zero and variance~$\Delta t$, the sample $\{r_1,\ldots,r_N\}$ is a sample of independent random variables each of which is normally distributed with mean $\mu-\sigma^2/2$ and variance $\sigma^2/\Delta t$.  We are focused on estimating $\sigma^2$, so it will simplify things to define
$$
y_i = r_i\sqrt{\Delta t} = \frac{\log S(t_i)-\log S(t_{i-1})}{\sqrt{\Delta t}}\;.
$$ {#eq-volyi}

The sample $\{y_1,\ldots,y_N\}$ is a sample of independent random variables each of which is normally distributed with mean $(\mu-\sigma^2/2)\sqrt{\Delta t}$ and variance $\sigma^2$.
As was discussed in the previous section, the best estimate of the mean of $y$ is the sample mean
$$\bar{y} = \frac{1}{N}\sum_{i=1}^{N}y_i\; ,$$
and the best estimate of $\sigma^2$ is
$$\hat{\sigma}^2 = \frac{1}{N-1}\sum_{i=1}^{N} (y_i-\bar{y})^2\; .$$
This means that we estimate $\mu$ as
$$\hat{\mu} = \frac{\bar{y}}{\sqrt{\Delta t}} + \frac{1}{2}\hat{\sigma}^2 = \bar{r}+ \frac{1}{2}\hat{\sigma}^2\; .$$ 

Let us digress for a moment to discuss the reliability of $\hat{\mu}$ as an estimate of $\mu$.  Notice that 
$$
\bar{r} 
= \frac{\sum_{i=1}^N \log S(t_i)-\log S(t_{i-1})}{N\Delta t}
$$
$$
=   \frac{\log S(T)-\log S(0)}{N\Delta t}
$$
$$
 = \frac{\log S(T)-\log S(0)}{T}\;.
$$ {#eq-volrbar}

Therefore the first component $\bar{r}$ of the estimate of $\mu$ depends only on the total change in $S$ over the time period.  Hence, the reliability of this component cannot depend on how frequently we observe $S$ within the time period $[0,T]$.  The standard deviation of $\bar{r}$ in repeated samples
is the standard deviation of $[\log S(T)-\log S(0)]/T$, which is $\sigma/\sqrt{T}$.  This is likely to be quite large.  For example, with $\sigma =0.3$ and ten years of data ($T=10$), the standard deviation of $\bar{r}$ is 9.5\%, which means that a 95\% confidence interval will be a band of roughly 38\%.  Given that $\mu$ itself should be of the order of magnitude of 10\%, such a wide confidence interval is useless for all practical purposes.

Fortunately, it is easier to estimate $\sigma$.  We observed in the previous section that the $\hat{\sigma}^2$ defined above can be calculated as
$$
\frac{1}{N-1}\sum_{i=1}^N y_i^2 - \frac{N\bar{y}^2}{N-1}\;.
$$ {#eq-estimator_sig2}

From the definition @eq-volyi of $y_i$ and equation @eq-volrbar, we have
$$\bar{y} =  \frac{\sqrt{\Delta t}}{T}[\log S(T)-\log S(0)]\; .$$
Hence, the second term in @eq-estimator_sig2 is
$$ \frac{N}{N-1}\left(\frac{\Delta t}{T^2}\right)[\log S(T)-\log S(0)]^2\; .$$
If we observe the stock price sufficiently frequently, so that $\Delta t$ is very small, this term will be negligible.  In this circumstance,  $\hat{\sigma}^2$ is approximately
$$
\frac{1}{N-1}\sum_{i=1}^N y_i^2 = \frac{1}{N-1}\sum_{i=1}^N \frac{[\log S(t_i)-\log S(t_{i-1})]^2}{\Delta t}
$$
$$
= \frac{N}{N-1}\times \frac{1}{T}\times \sum_{i=1}^N [\log S(t_i)-\log S(t_{i-1})]^2 \;.
$$ {#eq-estimator_sig2_3}

If we observe $S$ more and more frequently, letting $\Delta t \rightarrow 0$ and $N \rightarrow \infty$, the sum 
$$\sum_{i=1}^N [\log S(t_i)-\log S(t_{i-1})]^2$$
will converge with probability one to $\sigma^2T$, as explained in @sec-s_quadraticvariation.  This implies that $\hat{\sigma}^2$ will converge to $\sigma^2$.  Thus, in theory, we can estimate $\sigma^2$ with any desired degree of precision by simply observing $S$ sufficiently frequently.  This is true no matter how short the overall time period $[0,T]$ may be.  

In practice, this doesn't work out quite so well.  If we observe minute-by-minute data, or we observe each transaction, much of the variation in the price~$S$ will be due to bouncing back and forth between the bid price and the ask price.  This is not really what we want to estimate, and this source of variation will be much less important if we look at weekly or even daily data.  So, there are practical limits to how frequently we should observe $S$.  Nevertheless, it is still true that, if $\sigma^2$ were truly constant, we could estimate it with a very high degree of precision.
In fact, we can estimate the volatility of a stock with enough precision to determine that it really isn't constant!  The real problem that we face is to estimate and model a changing volatility.  

## Estimating a Changing Volatility
Without attempting yet to model how the volatility may change, we can say a few things about how we might estimate a changing volatility.  In this and following sections, we will take the observation interval $\Delta t$ to be fixed.  We assume it is small (say, a day or a week) and focus on the estimate @eq-estimator_sig2_3.  Recall from @sec-s_statistics that the reason we are dividing by $N-1$ rather than~$N$ is that the sample standard deviation usually underestimates the actual standard deviation, because it uses the sample mean, which will be closer to the points~$x_i$ than will be the true mean.  However, @eq-estimator_sig2_3 does not employ the sample mean (it replaces it with zero), so there is no reason to make this correction.  So, we take as our point of departure the estimate
$$\frac{1}{T} \sum_{i=1}^N [\log S(t_i)-\log S(t_{i-1})]^2 = \frac{1}{N}\sum_{i=1}^N y_i^2 \; .$$
An obvious response to the volatility changing over time is simply to avoid using data from the distant past.  Such data is not likely to be informative about the current value of the volatility.  What distant should mean in this context is not entirely clear, but, for example, we might want to use only the last 60 observations.  If we are using daily data, this would mean that at the end of each day we would add that day's observation and drop the observation from 61 days past.  This leads to a somewhat abruptly varying estimate.  For example, a very large movement in the price on a particular day increases the volatility estimate for the next 60 days.  On the 61st day, this observation would drop from the sample, leading to an abrupt drop in the estimate (presuming that there is not an equally large change in $S$ on the 61st day).  This seems unreasonable.  An estimate in which the impact of each observation decays smoothly over time is more attractive.  

We can construct such an estimate as
$$
\hat{\sigma}^2_{i+1} = (1-\lambda) y_{i}^2 + \lambda\hat{\sigma}^2_{i}
$$ {#eq-sig_estimator4}

for any constant $0<\lambda<1$.
Here, $\hat{\sigma}^2_{i+1}$ denotes the estimate of the volatility from date $t_{i}$ to date $t_{i+1}$.  The estimate @eq-sig_estimator4 is a weighted average of the estimate $\hat{\sigma}^2_{i}$ for the previous time period and the most recently observed squared change $y_{i}^2$.  Following the same procedure, the next estimate will be
$$
\hat{\sigma}^2_{i+2} = (1-\lambda) y_{i+1}^2 + \lambda\hat{\sigma}^2_{i+1}
$$
$$
= (1-\lambda) y_{i+1}^2 + \lambda(1-\lambda)  y_{i}^2 + \lambda^2\hat{\sigma}^2_{i}\;.
$$
Likewise, the estimate at the following date will be
$$\hat{\sigma}^2_{i+3} = (1-\lambda) y_{i+2}^2 +\lambda(1-\lambda) y_{i+1}^2 + \lambda^2(1-\lambda)^2  y_{i}^2 +\lambda^{3}\hat{\sigma}^2_{i}\; .$$
This demonstrates the declining importance of the squared deviation $y_{i}^2$ for future estimates.  At each date, $y_{i}^2$ enters with a weight that is lower by a factor of $\lambda$, compared to the previous date. 
If $\lambda$ is small, the decay in the importance of each squared deviation will be fast.  In fact, the formula @eq-sig_estimator4 shows that, if $\lambda$ is close to zero, the estimate $\hat{\sigma}_{i+1}^2$ is approximately equal to the squared deviation $y_i^2$---previous squared deviations are relatively unimportant.  On the other hand, if $\lambda$ is close to one, the decay will be slow; i.e., the importance of $y_i^2$ for the estimate $\hat{\sigma}^2_{i+2}$ will be nearly the same as for $\hat{\sigma}^2_{i+1}$, and nearly the same for $\hat{\sigma}^2_{i+3}$ as for $\hat{\sigma}^2_{i+2}$, etc.   This will lead to a smooth (slowly varying) volatility estimate.  The slowly varying nature of the estimate in this case is also clear from @eq-sig_estimator4, because it shows that if $\lambda$ is close to one, then $\hat{\sigma}^2_{i+1}$ will be approximately the same as $\hat{\sigma}^2_{i}$.

This method can also be used to estimate covariances, simply by replacing the squared deviations $y_i^2$ by the product of deviations for two different assets.  And, of course, given covariance and variance estimates, we can construct estimates of correlations.  To ensure that an estimated correlation is between $-1$ and $+1$, we will need to use the same $\lambda$ to estimate each of the variances and the covariance.  This is the method used by RiskMetrics.^[See Mina and Xiao [@MX], available online at \emph{www.riskmetrics.com].} \index{RiskMetrics}

## GARCH Models {#sec-s_garch}

We are going to adopt a subtle but important change of perspective now.  Instead of considering @eq-sig_estimator4 as simply an estimation procedure, we are going to assume that the actual volatility evolves according to @eq-sig_estimator4, or a generalization thereof.  We are also going to reintroduce the expected change in $\log S$, which we dropped in going from @eq-estimator_sig2 to @eq-estimator_sig2_3.  Specifically, we return to @eq-dlogs, but we operate under the risk-neutral measure, so 
$\mu=r-q$, and we have
$$
\log S(t_{i+1}) - \log S(t_i) = \left(r-q-\frac{1}{2}\sigma_{i+1}^2\right)\Delta t + \sigma_{i+1} \Delta B\;.
$$ {#eq-dlogs2}

We assume the volatility $\sigma_{i+1}$ between dates $t_i$ and $t_{i+1}$ is given by
$$
\sigma_{i+1}^2 = a + b y_{i}^2 + c \sigma_i^2\;,
$$ {#eq-garch}

for some constants $a > 0$, $b\geq 0$ and $c\geq 0$, with $y_i$ now defined by
$$y_i = \frac{\log S(t_i)-\log S(t_{i-1})-\left(r-q-\frac{1}{2}\sigma_i^2\right)\Delta t}{\sqrt{\Delta t}}\; .$$
From @eq-dlogs2, applied to the period from $t_{i-1}$ to $t_i$, this implies that $y_i$ is normally distributed with mean zero and variance $\sigma_i^2$, and of course $y_{i+1}$ has variance $\sigma_{i+1}^2$, etc.  
Under these assumptions, the random process $\log S$ is called a \index{GARCH process} GARCH(1,1) process.^[GARCH is the acronym for Generalized Autoregressive Conditional Heteroskedastic.  GARCH(1,1) means that there is only one past $y$  (no $y_{i-1]$, $y_{i-2}$, etc.) and one past $\sigma$ (no  $\sigma_{i-1}$, $\sigma_{i-2}$, etc.) in  @eq-garch.  See Bollerslev [@Bollerslev].}  There are many varieties of GARCH processes that have been proposed in the literature, but we will only consider GARCH(1,1), which is the simplest.

We assume $b+c<1$, in which case we can write the variance equation as a generalization of @eq-sig_estimator4.  Namely,
%$$\sigma_{i+1}^2 = (1-\phi)d + \phi\left[(1-\lambda) y_{i}^2 + \lambda \sigma^2_{i}\right]\; ,$$
$$
\sigma_{i+1}^2 = \kappa\theta + (1-\kappa)\left[  (1-\lambda) y_{i}^2 + \lambda\sigma^2_{i}\right]\;,
$$ {#eq-garch10}

where $\lambda=c/(b+c)$, 
%$\phi=b+c$, and $d=a/(1-b-c)$.  
$\kappa = 1-b-c$, and $\theta=a/(1-b-c)$.  Hence, $\sigma_{i+1}^2$ is a weighted average with weights $\kappa$ and $1-\kappa$, of two parts, one being the constant $\theta$ and the other being itself a weighted average of $y_{i}^2$ and $\sigma^2_{i}$.  Whatever the variance might be at time $t_i$, the variance of $y_j$ at any date $t_j$ far into the future, computed without knowing the intervening $y_{i+1}, y_{i+2},\ldots$, will be approximately the constant $\theta$.  The constant $\theta$ is called the unconditional variance, \index{unconditional variance} whereas $\sigma_{i}^2$ is the conditional variance of $y_i$.  \index{conditional variance}

To understand the unconditional variance, it is useful to consider the variance forecasting equation.  Specifically, we can calculate $E_{t_i} \left[\sigma_{i+n}^2\right]$, which is the estimate made at date $t_i$ of the variance of $y_{i+n}$; i.e, we estimate the variance without having observed $y_{i+1},\ldots,y_{i+n-1}$.  Note that by definition $E_{t_{i}}[y_{i+1}^2]=\sigma_{i+1}^2$, so @eq-garch10 implies
$$
E_{t_{i}}\left[\sigma_{i+2}^2\right] = \kappa\theta + (1-\kappa)\left[  (1-\lambda) E_{t_{i}}[y_{i+1}^2] + \lambda\sigma^2_{i+1}\right] 
$$
$$
= \kappa\theta + (1-\kappa)\sigma^2_{i+1}\; .
$$
Likewise,
$$E_{t_{i+1}}\left[\sigma_{i+3}^2\right] = \kappa\theta + (1-\kappa)\sigma^2_{i+2}\; ,$$
and taking the expectation at date $t_i$ of both sides of this yields
$$
E_{t_{i}}\left[\sigma_{i+3}^2\right] = E_{t_{i}}\left[E_{t_{i+1}}\left[\sigma_{i+3}^2\right]\right] =\kappa\theta + (1-\kappa)E_{t_{i}}\left[\sigma_{i+2}^2\right]
$$
$$
=\kappa\theta + (1-\kappa)\left[\kappa\theta + (1-\kappa)\sigma^2_{i+1}\right]
$$
$$
=\kappa\theta[1+(1-\kappa)] + (1-\kappa)^2\sigma^2_{i+1}\;.
$$
This generalizes to
$$E_{t_{i}}\left[\sigma_{i+n}^2\right] = \kappa\theta\left[1+(1-\kappa)+ \cdots (1-\kappa)^{n-2}\right] + (1-\kappa)^{n-1}\sigma^2_{i+1}\; .$$
Thus, there is decay at rate $\kappa$ in the importance of the current volatility $\sigma^2_{i+1}$ for forecasting the future volatility.  Furthermore, as $n\rightarrow \infty$, the geometric series
$$1+(1-\kappa)+ \cdots (1-\kappa)^{n-2}$$
converges to $1/\kappa$, so, as $n \rightarrow \infty$ we obtain
$$E_{t_{i}}\left[\sigma_{i+n}^2\right] \rightarrow \theta\; .$$
This means that our best estimate of the conditional variance, at some date far in the future, is approximately the unconditional variance $\theta$.

The most interesting feature of the volatility equation is that large returns (in absolute value) lead to an increase in the variance and hence are likely to be followed by more large returns (whether positive or negative).  This is the phenomenon of volatility clustering, \index{volatility clustering} which is quite observable in actual markets.  This feature also implies that the distribution of returns will be fat tailed  (more technically, leptokurtic).  \index{leptokurtic} This means that the probability of extreme returns is higher than under a normal distribution with the same standard deviation.^[Conversely, the probability of returns very near the mean must also be higher than under a normal distribution with the same standard deviation---a fat-tailed distribution must also have a relatively narrow peak.]  It is well documented that daily and weekly returns in most markets have this fat-tailed property.

We can simulate a path of an asset price that follows a GARCH process and the path of its volatility as follows.  The following macro produces three columns of data (with headings), the first column being time, the second  the asset price, and the third  the volatility.

\addcontentsline{lof}{figure}{Simulating GARCH}
\begin{verbatim}
Sub Simulating_GARCH()
Dim S, sigma, r, q, dt, theta, kappa, lambda, LogS, Sqrdt
Dim a, b, c, y, i, N
S = InputBox("Enter initial stock price")
sigma = InputBox("Enter initial volatility")
r = InputBox("Enter risk-free rate")
q = InputBox("Enter dividend yield")
dt = InputBox("Enter length of each time period (Delta t)")
N = InputBox("Enter number of time periods (N)")
theta = InputBox("Enter theta")
kappa = InputBox("Enter kappa")
lambda = InputBox("Enter lambda")
LogS = Log(S)
Sqrdt = Sqr(dt)
a = kappa * theta
b = (1 - kappa) * (1 - lambda)
c = (1 - kappa) * lambda
ActiveCell.Value = "Time"
ActiveCell.Offset(0, 1) = "Stock Price"
ActiveCell.Offset(0, 2) = "Volatility"
ActiveCell.Offset(1, 0) = 0               ' initial time
ActiveCell.Offset(1, 1) = S               ' initial stock price
ActiveCell.Offset(1, 2) = sigma           ' initial volatility
For i = 1 To N
    ActiveCell.Offset(i + 1, 0) = i * dt  ' next time
    y = sigma * RandN()
    LogS = LogS + (r - q - 0.5 * sigma * sigma) * dt + Sqrdt * y
    S = Exp(LogS)
    ActiveCell.Offset(i + 1, 1) = S       ' next stock price
    sigma = Sqr(a + b * y ^ 2 + c * sigma ^ 2)
    ActiveCell.Offset(i + 1, 2) = sigma   ' next volatility
Next i
End Sub
\end{verbatim}

To price European options, we need to compute the usual probabilities 
$\text{prob}^S(S(T)>K)$ and $\text{prob}^R(S(T) >K)$.
Heston and Nandi [@HN] provide a fast method for computing these probabilities in a GARCH (1,1) model.^[Actually, a slightly more general model is considered in [@HN], in which large negative returns lead to a greater increase in volatility than do large positive returns.  This accommodates the empirically observed negative correlation between stock returns and volatility.]  Rather than developing this approach, we will show in @sec-c_introcomputation how to apply Monte-Carlo methods.



## Stochastic Volatility Models {#sec-s_stochasticvolatility}

The volatility is stochastic (random) in a GARCH model, but it is determined by the changes in the stock price.  In this section, in contrast, we will consider models in which the volatility depends on a second Brownian motion.  \index{stochastic volatility} The most popular model of this type is the model of Heston [@Heston].  \index{Heston model} In this model, we have, as usual, 

$$
\mathrm{d}\log S = \left(r-q-\frac{1}{2}\sigma^2\right)\,\mathrm{d} t + \sigma\,\mathrm{d} B_s\;,
$$ {#eq-heston1}

where $B_s$ is a Brownian motion under the risk-neutral measure but now $\sigma$ is not a constant but instead evolves as $\sigma(t) = \sqrt{v(t)
$$$, where}

dv(t) = \kappa \big[\theta-v(t)\big]\,\mathrm{d} t + \gamma \sqrt{v(t)}\,\mathrm{d} B_v\;,
$$ {#eq-heston2}

where $B_v$ is a Brownian motion under the risk-neutral measure having a constant correlation $\rho$ with the Brownian motion $B_s$.  In this equation, $\kappa$, $\theta$ and~$\gamma$ are positive constants.  Given the empirical fact that negative return shocks have a bigger impact on future volatility than do positive shocks, one would expect the correlation $\rho$ to be negative.

The term $\kappa (\theta-v)$ will be positive when $v<\theta$ and negative when $v>\theta$ and hence $\sigma^2=v$ will tend to drift towards $\theta$, which, as in the GARCH model, is the long-run or unconditional mean of $\sigma^2$.  Thus, the volatility is said to mean revert. \index{mean reversion} The rate at which it drifts towards $\theta$ is obviously determined by the magnitude of $\kappa$, also as in the GARCH model.  

The specification @eq-heston2 implies that the volatility of $v$ approaches zero whenever $v$ approaches zero.      In this circumstance, one might expect the drift towards $\theta$ to dominate the volatility and keep $v$ nonnegative, and this is indeed the case; thus, the definition $\sigma(t) = \sqrt{v(t)}$ is possible.  Moreover, the parameter $\gamma$ plays a role here that is similar to the role of $1-\lambda$ in the GARCH model---the variance of the variance in the GARCH model @eq-garch10 depends on the weight $1-\lambda$ placed on the scaled return $y_i$, just as the variance of the variance in the stochastic volatility model @eq-heston2 depends on the weight $\gamma$ placed on  $\mathrm{d} B_v$.

We could discretize @eq-heston12combined as:

$$
\log S(t_{i+1}) = \log S(t_i) + \left(r-q-\frac{1}{2}\sigma(t_i)^2\right)\,\Delta t + \sqrt{v(t_i)}\,\Delta B_s,
$$ {#eq-heston3}

$$
v(t_{i+1}) = v(t_i) + \kappa \big[\theta-v(t_i)\big]\,\Delta t + \gamma \sqrt{v(t_i)}\,\Delta B_v\;.
$$ {#eq-heston4}


However, even though in the continuous-time model @eq-heston12combined we always have $v(t) \geq 0$ and hence can define $\sigma(t)=\sqrt{v(t)}$, there is no guarantee that $v(t_{i+1})$ defined by @eq-heston4 will be nonnegative.  A simple remedy is to define $v(t_{i+1})$ as the larger of zero and the right-hand side of @eq-heston4; thus, we will simulate the Heston model as @eq-heston3 and^[There are better (but more complicated) ways to simulate the Heston model.  An excellent discussion of ways to simulate the volatility process can be found in Glasserman [@Glasserman].  Broadie and Kaya [@broadiekaya] present a method for simulating from the exact distribution of the asset price in the Heston model and related models.]
$$
v(t_{i+1}) = \max\left\{0,v(t_i) + \kappa \big[\theta-v(t_i)\big]\,\Delta t + \gamma \sqrt{v(t_i)}\,\Delta B_v\right\}\;.
$$ {#eq-heston41}



A simple way to simulate the changes $\Delta B_s$ and $\Delta B_v$ in the two correlated Brownian motions is to generate two independent standard normals $z_1$ and~$z_2$ and take
$$\Delta B_s = \sqrt{\Delta t}\,z \qquad \text{and} \qquad \Delta B_v = \sqrt{\Delta t}\,z^*\; ,$$
where we define 
$$z = z_1 \qquad \text{and} \qquad z^* = \rho z_1 + \sqrt{1-\rho^2}\,z_2\; .$$
The random variable $z^*$ is also a standard normal, and the correlation between~$z$ and $z^*$ is~$\rho$.  

\addcontentsline{lof}{figure}{Simulating Stochastic Volatility}
\begin{verbatim}
Sub Simulating_Stochastic_Volatility()
Dim S, sigma, r, q, dt, theta, kappa, Gamma, rho, LogS, var
Dim Sqrdt, Sqrrho, z1, Z2, Zstar, i, N
S = InputBox("Enter initial stock price")
sigma = InputBox("Enter initial volatility")
r = InputBox("Enter risk-free rate")
q = InputBox("Enter dividend yield")
dt = InputBox("Enter length of each time period (Delta t)")
N = InputBox("Enter number of time periods (N)")
theta = InputBox("Enter theta")
kappa = InputBox("Enter kappa")
Gamma = InputBox("Enter gamma")
rho = InputBox("Enter rho")
LogS = Log(S)
var = sigma * sigma
Sqrdt = Sqr(dt)
Sqrrho = Sqr(1 - rho * rho)
ActiveCell.Value = "Time"
ActiveCell.Offset(0, 1) = "Stock Price"
ActiveCell.Offset(0, 2) = "Volatility"
ActiveCell.Offset(1, 0) = 0               ' initial time
ActiveCell.Offset(1, 1) = S               ' initial stock price
ActiveCell.Offset(1, 2) = sigma           ' initial volatility
For i = 1 To N
    ActiveCell.Offset(i + 1, 0) = i * dt  ' next time
    z1 = RandN()
    LogS = LogS + (r-q-0.5*sigma*sigma)*dt + sigma*Sqrdt*z1
    S = Exp(LogS)
    ActiveCell.Offset(i + 1, 1) = S       ' next stock price
    Z2 = RandN()
    Zstar = rho * z1 + Sqrrho * Z2
    var = Application.Max(0, var+kappa*(theta-var)*dt _
           +Gamma*sigma*Sqrdt*Zstar)
    sigma = Sqr(var)
    ActiveCell.Offset(i + 1, 2) = sigma   ' next volatility
Next i
End Sub
\end{verbatim}


To price European options, we again need to compute 
$$\text{prob}^S(S(T)>K) \qquad \text{and} \qquad \text{prob}^R(S(T)>K)\; .$$
The virtue of modelling volatility as in @eq-heston2 is that these probabilities can be computed quite efficiently, as shown by Heston [@Heston].^[Further discussion can be found in Epps [@Epps].]  There are many other ways in which one could model volatility, but the computations may be more difficult.  For example, one could  replace @eq-heston2 by
$$
\sigma(t) = \mathrm{e}^{v(t)} \quad \text{and} \quad dv(t) = \kappa (\theta-v(t))\,\mathrm{d} t + \lambda \,\mathrm{d} B^*\;.
$$ {#eq-heston5}

This implies a lognormal volatility and is simpler to simulate than @eq-heston2---because $\mathrm{e}^{v}$ is well defined even when $v$ is negative---but it is easier to calculate the probabilities $\text{prob}^S(S(T)>K)$ and $\text{prob}^R(S(T)>K)$ if we assume @eq-heston2.

One way to implement the GARCH or stochastic volatility model is to imply both the initial volatility $\sigma(0)$ and the constants $\kappa$, $\theta$ and $\lambda$ or $\kappa$, $\theta$, $\gamma$ and $\rho$ from observed option prices.  These four (or five) constants can be computed by forcing the model prices of four (or five) options to equal the observed market prices.  Or, a larger set of prices can be used and the constants can be chosen to minimize the average squared error or some other measure of goodness-of-fit between the model and market prices.

## Smiles and Smirks Again {#sec-s_smilesagain}

As mentioned before, the GARCH and stochastic volatility models can generate  fat-tailed distributions for the asset price $S(T)$.  Thus, they can be more nearly consistent with the option smiles \index{smile}discussed in @sec-s_smiles than is the Black-Scholes model (though it appears that one must include jumps in asset prices as well as stochastic volatility in order to duplicate market prices with an option pricing formula).  To understand the relation, let $\sigma_\text{am}$ denote the implied volatility from an at-the-money call option, i.e., a call option with strike $K=S(0)$.  The characteristic of a smile is that implied volatilities from options of the same maturity with strike prices significantly above and below $S(0)$ are higher than $\sigma_\text{am}$.  

A strike price higher than $S(0)$  corresponds to an out-of-the money call option.  The high implied volatility means that the market is pricing the right to buy at $K>S(0)$ above the Black-Scholes price computed from the volatility~$\sigma_\text{am}$; thus, the market must attach a higher probability to stock prices $S(T)>S(0)$ than the  volatility $\sigma_\text{am}$ would suggest.  

A strike price lower than $S(0)$ corresponds to an in-the-money call option.  The put option with the same strike is out of the money.  The high implied volatility means that the market is pricing call options  above the Black-Scholes price computed from the volatility $\sigma_\text{am}$.  By put-call parity, the market must also be pricing put options above the Black-Scholes price computed from the  volatility $\sigma_\text{am}$.  The high prices for the rights to buy and sell at $K<S(0)$ means that the market must attach a higher probability to stock prices $S(T)<S(0)$ than the volatility $\sigma_\text{am}$ would suggest.  In particular, the high price for the right to sell at $K<S(0)$ means a high insurance premium for owners of the asset who seek to insure their positions, which is consistent with a market view that there is a significant probability of a large loss.  This can be interpreted as a crash premium.  \index{crash premium} Indeed, the implied volatilities at strikes less than $S(0)$  are typically higher than the implied volatilities at strikes above $S(0)$ (giving the smile the appearance of a smirk, as discussed in @sec-s_smiles), which is consistent with a larger probability of crashes than of booms (a fatter tail for low returns than for high).



## Hedging and Market Completeness

The GARCH model is inherently a discrete-time model.  If returns have a GARCH structure at one frequency (e.g., monthly), they will not have a GARCH structure at a different frequency (e.g., weekly).  Hence, the return period (monthly, weekly, \ldots) is part of the specification of the model.  One interpretation of the model is that the dates $t_i$ at which the variance changes are the only dates at which investors can trade.  Under this interpretation, it is impossible to perfectly hedge an option: the gross return $S(t_i)/S(t_{i-1})$ over the interval $(t_{i-1},t_i)$ is lognormally distributed, so no portfolio of the stock and riskless asset formed at $t_{i-1}$ and held over the interval $(t_{i-1},t_i)$ can perfectly replicate the return of an option over the interval.  As discussed in @sec-s_incomplete, we call a market in which some derivatives cannot be perfectly hedged an incomplete market. \index{incomplete market}  Thus, the GARCH model is an example of an incomplete market, if investors can only trade at the frequency at which returns have a GARCH structure.  However, it is unreasonable to assume that investors can only trade weekly or monthly or even daily.

Another interpretation of the GARCH model is that investors can trade continuously and the asset has a constant volatility within each period $(t_{i-1},t_i)$.  Under this interpretation, the market is complete and options can be delta-hedged.  The completeness is a result of the fact that the change $\sigma_{i+1}-\sigma_i$ in the volatility  at date $t_i$ (recall that $\sigma_i$ is the volatility over the period $(t_{i-1},t_i)$ and $\sigma_{i+1}$ is the volatility over the period $(t_{i},t_{i+1})$) depends only on $\log S(t_i)$.  Thus, the only random factor in the model that needs to be hedged is, as usual, the underlying asset price.  However, this interpretation of the model is also a bit strange.  Suppose for example that monthly returns are assumed to have a GARCH structure.  Then the model states that the volatility in February will be higher if there is an unusually large return (in absolute value) in January.  Suppose there is an unusually large return in the first half of January.  Then, intuitively, one would expect the change in the volatility to occur in the second half of January rather than being delayed until February.  However, the model specifies that the volatility is constant during each month, hence constant during January in this example.

The stochastic volatility model is more straightforward.  The market is definitely incomplete.  The value of a call option at date $t<T$, where $T$ is the maturity of the option, will depend on the underlying asset price $S(t)$ and the volatility $\sigma(t)$.  Denoting the value by $C(t,S(t),\sigma(t))$, we have
from Ito's formula that
$$
\mathrm{d} C(t) = \text{something}\;\mathrm{d} t + \frac{\partial C}{\partial S}\,\mathrm{d} S(t) + \frac{\partial C}{\partial \sigma}\,\mathrm{d} \sigma(t)\; .$$
A replicating portfolio must have the same dollar change at each date $t$.  If we hold $\partial C/\partial S$ shares of the underlying asset, then the change in the value of the shares will be $(\partial C/\partial S)\,\mathrm{d} S$.  However, there is no way to match the  
$(\partial C/\partial \sigma)\,\mathrm{d} \sigma$ term using the underlying asset and the riskless asset.  

The significance of the market being incomplete is that the value of a derivative asset that cannot be replicated using traded assets (e.g., the underlying and riskless assets) is not uniquely determined by arbitrage considerations.  As discussed in @sec-s_incomplete, one must use equilibrium pricing in this circumstance.  \index{equilibrium pricing} That is what we have implicitly done in this chapter.  By assuming particular dynamics for the volatility under the risk-neutral measure, we have implicitly selected a particular risk-neutral measure from the set of risk-neutral measures that are consistent with the absence of arbitrage.  


## Exercises

::: {#exr-e_mixture}
  The purpose of this exercise is to generate a fat-tailed distribution from a model that is simpler than the GARCH and stochastic volatility models but has somewhat the same flavor.  The distribution will be a mixture of normals. Create an Excel worksheet in which the user can input $S$, $r$, $q$, $T$, $\sigma_1$ and $\sigma_2$.  Use these inputs to produce a column of 500 simulated $\log S(T)$.  In each simulation, define $\log S(T)$ as
$$\log S(T) = \log S(0) + \left(r-q-\frac{1}{2}\sigma^2\right)T + \sigma \sqrt{T}z\;,$$
where $z$ is a standard normal,
$\sigma = x\sigma_1 + (1-x)\sigma_2$,
and $x$ is a random variable that equals zero or one with equal probabilities.  You can define $z$ in each simulation as
\verb!NormSInv(Rand())! and $x$ as \verb!If(Rand()<0.5,1,0)!. 
Calculate the mean and standard deviation of the $\log S(T)$ and calculate the fraction that lie more than two standard deviations below the mean.  If the $\log S(T)$ all came from a normal distribution with the same variance, then this fraction should equal $\mathrm{N}(-2) = $ 2.275\%.  If the fraction is higher, then the distribution is fat tailed.  (Of course, the actual fraction would differ from 2.275\% in any particular case due to the randomness of the simulation, even if all of the $\log S(T)$ came from a normal distribution with the same variance).
:::

::: {#exr-e_GARCH1}
  Create an Excel macro prompting the user to input the same inputs as in the \verb!Simulating_GARCH! subroutine except for the initial volatility and $\theta$.  Simulate 500 paths of a GARCH process and output $\log S(T)$ for each simulation (you don't need to output the entire paths as in the \verb!Simulating_GARCH! macro).  Take the initial volatility to be 0.3 and $\theta = 0.09$.  Determine whether the distribution is fat-tailed by computing the fraction of the $\log S(T)$ that lie two or more standard deviations below the mean, as in the previous exercise.  For what values of $\kappa$ and $\lambda$ does the distribution appear to be especially fat-tailed? 
:::
::: {#exr-nolabel}
 Repeat Prob.~\ref{e_GARCH1} for the Heston stochastic volatility model, describing the values of $\kappa$,  $\gamma$ and $\rho$ that appear to generate especially fat-tailed distributions.
\mathrm{n}ext
\underline{Note}
\mathrm{n}ext
Excel provides some tools that are useful for exercises of this sort.  If you load the Data Analysis add-in (click Tools/Add Ins), you can produce a histogram of the simulated data, which is useful for visually analyzing departures from normality.  The Data Analysis add-in will also produce summary statistics, including the kurtosis and skewness of the data (without using the add-in, the kurtosis can be computed with the Excel function \verb!KURT! and the skewness with the Excel function \verb!SKEW!).  The kurtosis of a random variable $x$ is defined as $E[(x-\mu)^4]/\sigma^4$, where $\mu$ is the mean and $\sigma$ is the standard deviation of~$x$.  The kurtosis of a normal distribution is 3.  A kurtosis larger than 3 is excess kurtosis, meaning the distribution is leptokurtic (fat tailed).  Excel's \verb!KURT! function actually computes excess kurtosis, so a positive value indicates a fat-tailed distribution.  The skewness of $x$ is defined as $E[(x-\mu)^3]/\sigma^3$.  The skewness of a normal distribution is zero.  Negative skewness indicates the distribution is skewed to the left, meaning the lower tail is fatter than the upper tail (crashes are more likely than booms).  Positive skewness indicates the distribution is skewed to the right.
:::
