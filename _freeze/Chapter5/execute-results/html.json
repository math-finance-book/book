{
  "hash": "8498becd45eb7f501da2a7dc01fb6b6e",
  "result": {
    "engine": "jupyter",
    "markdown": "# Stochastic Volatility {#sec-c_stochasticvolatility}\n\nThus far, we have assumed that the volatility of the underlying asset is constant or varying in a non-random way during the lifetime of the derivative.  In this chapter we will look at models that relax this assumption and allow the volatility to change randomly.  This is very important, because there is plenty of evidence that volatilities do change over time in a random way.  \n\nIn the first three sections, we will consider the problem of estimating the volatility.  The discussion of estimation methods leads naturally into the discussion of modeling a changing volatility.  \n\n## Statistics Review {#sec-s_statistics}\n\nWe begin with a brief review of basic statistics.\nGiven a random sample $\\{x_1,\\ldots,x_N\\}$ of size $N$ from a population with mean $\\mu$ and variance $\\sigma^2$, the best estimate of $\\mu$ is of course the sample mean \n$$\\bar{x} = \\frac{1}{N}\\sum_{i=1}^{N}x_i\\; .$$\nThe variance is the expected value of $(x-\\mu)^2$, so an obvious estimate of the variance is the sample average of $(x_i-\\mu)^2$, replacing $\\mu$ with its estimate $\\bar{x}$.  This would be\n$$\\frac{1}{N}\\sum_{i=1}^{N} (x_i-\\bar{x})^2$$\nHowever, because $\\bar{x}$ is computed from the $x_i$, the $x_i$ will deviate less on average from $\\bar{x}$ than they do from the true mean $\\mu$.  Hence the estimate proposed above will on average be less than $\\sigma^2$.  To eliminate this bias, it suffices just to scale the estimate up by a factor of $N/(N-1)$.  This leads to the estimate\n$$s^2=\\frac{1}{N-1}\\sum_{i=1}^{N} (x_i-\\bar{x})^2\\; ,$$\nand the best estimate of $\\sigma$ is the square root\n$$s=\\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N} (x_i-\\bar{x})^2}\\; .$$\nTo calculate $s^2$, notice that \n\\begin{align*}\n\\sum_{i=1}^{N} (x_i-\\bar{x})^2 &= \\sum_{i=1}^{N} (x_i^2-2x_i\\bar{x}+\\bar{x}^2)\\\\\n&=\\sum_{i=1}^{N} x_i^2 -2\\bar{x}\\sum_{i=1}^{N} x_i + \\sum_{i=1}^N \\bar{x}^2\\\\\n&=\\sum_{i=1}^{N} x_i^2 -2\\bar{x}(N\\bar{x})+N\\bar{x}^2\\\\\n&=\\sum_{i=1}^{N} x_i^2 -N\\bar{x}^2\\;.\n\\end{align*}\nTherefore\n$$s=\\sqrt{\\frac{1}{N-1}\\left(\\sum_{i=1}^{N} x_i^2-N\\bar{x}^2\\right)}\\; .$$\n\nIt is important to know how much variation there would be in $\\bar{x}$ if one had access to multiple random samples.  More variation means that an $\\bar{x}$ computed from a single sample will be a less reliable estimate of $\\mu$.  The variance of $\\bar{x}$ in repeated samples is $\\sigma^2/N$,^[The variance of $\\bar{x} = (1/N)(x_1 + \\cdots + x_N)$ is, by independence of the $x_i$, equal to $(1/N)^2(\\mathrm{var}{x_1} + \\cdots + \\mathrm{var}{x_N})$, and, because the $x_i$ all have the same variance $\\sigma^2$, this is equal to $(1/N)^2 \\times N\\sigma^2 = \\sigma^2/N$.] and our best estimate of this variance is $s^2/N$.  The standard deviation of $\\bar{x}$ in repeated samples, which is called the standard error of \\index{standard error} $\\bar{x}$, is $\\sigma/\\sqrt{N}$, and we estimate this by $s/\\sqrt{N}$, which equals\n$$\\sqrt{\\frac{1}{N(N-1)}\\left(\\sum_{i=1}^{N} x_i^2-N\\bar{x}^2\\right)}\\; .$$\nIf the population from which $x$ is sampled has a normal distribution, then a 95\\% confidence interval for $\\mu$ will be $\\bar{x}$ plus or minus 1.96 standard errors.  Even if $x$ does not have a normal distribution, by the Central Limit Theorem, $\\bar{x}/\\sqrt{N}$ will be approximately normally distributed if the sample size $N$ is large enough, and plus or minus 1.96 standard errors will still be approximately a 95\\% confidence interval for $\\mu$. \\index{confidence interval}\n\n## Estimating a Constant Volatility and Mean {#sec-s_estimatingvolatility}\n\nConsider an asset price that is a geometric Brownian motion under the actual probability measure:\n$$\\frac{\\mathrm{d} S}{S} = \\mu\\,\\mathrm{d} t + \\sigma\\,\\mathrm{d} B\\; ,$$\nwhere $\\mu$ and $\\sigma$ are unknown constants and $B$ is a Brownian motion.  We can as usual write this in log form as\n$$\\mathrm{d}\\log S = \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)\\,\\mathrm{d} t + \\sigma\\,\\mathrm{d} B\\; .$$\nOver a discrete time period of length $\\Delta t$, this implies\n$$\n\\Delta \\log S = \\left(\\mu-\\frac{1}{2}\\sigma^2\\right)\\Delta t + \\sigma \\Delta B\\;.\n$$ {#eq-dlogs}\n\nSuppose we have observed the asset price $S$ at dates $0=t_0<t_1<\\cdots< t_N=T$, where $t_i-t_{i-1}=\\Delta t$.  If the asset pays dividends, we will take $S$ to be the value of the portfolio in which the dividends are reinvested in new shares.  Thus, in general, $S(t_i)/S(t_{i-1})$ denotes the gross return (one plus the rate of return) between dates $t_{i-1}$ and $t_i$.  This return is measured on a non-compounded and non-annualized basis.  The annualized continuously-compounded rate of return is the rate $r_i$ defined by \n$$\\frac{S(t_i)}{S(t_{i-1})} = \\mathrm{e}^{r_i\\Delta t}\\; .$$\nThis implies that\n$$\nr_i = \\frac{\\log S(t_i)-\\log S(t_{i-1})}{\\Delta t} = \\mu-\\frac{1}{2}\\sigma^2 + \\sigma \\frac{B(t_i)-B(t_{i-1})}{\\Delta t}\\;.\n$$ {#eq-contcompreturn}\n\nBecause $B(t_i)-B(t_{i-1})$ is normally distributed with mean zero and variance $\\Delta t$, the sample $\\{r_1,\\ldots,r_N\\}$ is a sample of independent random variables each of which is normally distributed with mean $\\mu-\\sigma^2/2$ and variance $\\sigma^2/\\Delta t$.  We are focused on estimating $\\sigma^2$, so it will simplify things to define\n$$\ny_i = r_i\\sqrt{\\Delta t} = \\frac{\\log S(t_i)-\\log S(t_{i-1})}{\\sqrt{\\Delta t}}\\;.\n$$ {#eq-volyi}\n\nThe sample $\\{y_1,\\ldots,y_N\\}$ is a sample of independent random variables each of which is normally distributed with mean $(\\mu-\\sigma^2/2)\\sqrt{\\Delta t}$ and variance $\\sigma^2$.\nAs was discussed in the previous section, the best estimate of the mean of $y$ is the sample mean\n$$\\bar{y} = \\frac{1}{N}\\sum_{i=1}^{N}y_i\\; ,$$\nand the best estimate of $\\sigma^2$ is\n$$\\hat{\\sigma}^2 = \\frac{1}{N-1}\\sum_{i=1}^{N} (y_i-\\bar{y})^2\\; .$$\nThis means that we estimate $\\mu$ as\n$$\\hat{\\mu} = \\frac{\\bar{y}}{\\sqrt{\\Delta t}} + \\frac{1}{2}\\hat{\\sigma}^2 = \\bar{r}+ \\frac{1}{2}\\hat{\\sigma}^2\\; .$$ \n\nLet us digress for a moment to discuss the reliability of $\\hat{\\mu}$ as an estimate of $\\mu$.  Notice that \n\n$$\n\\bar{r} \n= \\frac{\\sum_{i=1}^N \\log S(t_i)-\\log S(t_{i-1})}{N\\Delta t}\n$$\n$$\n=   \\frac{\\log S(T)-\\log S(0)}{N\\Delta t}\n$$\n$$\n = \\frac{\\log S(T)-\\log S(0)}{T}\\;.\n$$ {#eq-volrbar}\n\n\nTherefore the first component $\\bar{r}$ of the estimate of $\\mu$ depends only on the total change in $S$ over the time period.  Hence, the reliability of this component cannot depend on how frequently we observe $S$ within the time period $[0,T]$.  The standard deviation of $\\bar{r}$ in repeated samples\nis the standard deviation of $[\\log S(T)-\\log S(0)]/T$, which is $\\sigma/\\sqrt{T}$.  This is likely to be quite large.  For example, with $\\sigma =0.3$ and ten years of data ($T=10$), the standard deviation of $\\bar{r}$ is 9.5\\%, which means that a 95\\% confidence interval will be a band of roughly 38\\%.  Given that $\\mu$ itself should be of the order of magnitude of 10\\%, such a wide confidence interval is useless for all practical purposes.\n\nFortunately, it is easier to estimate $\\sigma$.  We observed in the previous section that the $\\hat{\\sigma}^2$ defined above can be calculated as\n$$\n\\frac{1}{N-1}\\sum_{i=1}^N y_i^2 - \\frac{N\\bar{y}^2}{N-1}\\;.\n$$ {#eq-estimator_sig2}\n\nFrom @eq-volyi of $y_i$ and @eq-volrbar, we have\n$$\\bar{y} =  \\frac{\\sqrt{\\Delta t}}{T}[\\log S(T)-\\log S(0)]\\; .$$\nHence, the second term in @eq-estimator_sig2 is\n$$ \\frac{N}{N-1}\\left(\\frac{\\Delta t}{T^2}\\right)[\\log S(T)-\\log S(0)]^2\\; .$$\nIf we observe the stock price sufficiently frequently, so that $\\Delta t$ is very small, this term will be negligible.  In this circumstance,  $\\hat{\\sigma}^2$ is approximately\n\n$$\n\\frac{1}{N-1}\\sum_{i=1}^N y_i^2 = \\frac{1}{N-1}\\sum_{i=1}^N \\frac{[\\log S(t_i)-\\log S(t_{i-1})]^2}{\\Delta t}\n$$\n$$\n= \\frac{N}{N-1}\\times \\frac{1}{T}\\times \\sum_{i=1}^N [\\log S(t_i)-\\log S(t_{i-1})]^2 \\;.\n$$ {#eq-estimator_sig2_3}\n\n\nIf we observe $S$ more and more frequently, letting $\\Delta t \\rightarrow 0$ and $N \\rightarrow \\infty$, the sum \n$$\\sum_{i=1}^N [\\log S(t_i)-\\log S(t_{i-1})]^2$$\nwill converge with probability one to $\\sigma^2T$, as explained in @sec-s_quadraticvariation.  This implies that $\\hat{\\sigma}^2$ will converge to $\\sigma^2$.  Thus, in theory, we can estimate $\\sigma^2$ with any desired degree of precision by simply observing $S$ sufficiently frequently.  This is true no matter how short the overall time period $[0,T]$ may be.  \n\nIn practice, this doesn't work out quite so well.  If we observe minute-by-minute data, or we observe each transaction, much of the variation in the price $S$ will be due to bouncing back and forth between the bid price and the ask price.  This is not really what we want to estimate, and this source of variation will be much less important if we look at weekly or even daily data.  So, there are practical limits to how frequently we should observe $S$.  Nevertheless, it is still true that, if $\\sigma^2$ were truly constant, we could estimate it with a very high degree of precision.\nIn fact, we can estimate the volatility of a stock with enough precision to determine that it really isn't constant!  The real problem that we face is to estimate and model a changing volatility.  \n\n## Estimating a Changing Volatility\nWithout attempting yet to model how the volatility may change, we can say a few things about how we might estimate a changing volatility.  In this and following sections, we will take the observation interval $\\Delta t$ to be fixed.  We assume it is small (say, a day or a week) and focus on the estimate @eq-estimator_sig2_3.  Recall from @sec-s_statistics that the reason we are dividing by $N-1$ rather than $N$ is that the sample standard deviation usually underestimates the actual standard deviation, because it uses the sample mean, which will be closer to the points $x_i$ than will be the true mean.  However, @eq-estimator_sig2_3 does not employ the sample mean (it replaces it with zero), so there is no reason to make this correction.  So, we take as our point of departure the estimate\n$$\\frac{1}{T} \\sum_{i=1}^N [\\log S(t_i)-\\log S(t_{i-1})]^2 = \\frac{1}{N}\\sum_{i=1}^N y_i^2 \\; .$$\nAn obvious response to the volatility changing over time is simply to avoid using data from the distant past.  Such data is not likely to be informative about the current value of the volatility.  What distant should mean in this context is not entirely clear, but, for example, we might want to use only the last 60 observations.  If we are using daily data, this would mean that at the end of each day we would add that day's observation and drop the observation from 61 days past.  This leads to a somewhat abruptly varying estimate.  For example, a very large movement in the price on a particular day increases the volatility estimate for the next 60 days.  On the 61st day, this observation would drop from the sample, leading to an abrupt drop in the estimate (presuming that there is not an equally large change in $S$ on the 61st day).  This seems unreasonable.  An estimate in which the impact of each observation decays smoothly over time is more attractive.  \n\nWe can construct such an estimate as\n$$\n\\hat{\\sigma}^2_{i+1} = (1-\\lambda) y_{i}^2 + \\lambda\\hat{\\sigma}^2_{i}\n$$ {#eq-sig_estimator4}\n\nfor any constant $0<\\lambda<1$.\nHere, $\\hat{\\sigma}^2_{i+1}$ denotes the estimate of the volatility from date $t_{i}$ to date $t_{i+1}$.  The estimate @eq-sig_estimator4 is a weighted average of the estimate $\\hat{\\sigma}^2_{i}$ for the previous time period and the most recently observed squared change $y_{i}^2$.  Following the same procedure, the next estimate will be\n\\begin{align*}\n\\hat{\\sigma}^2_{i+2}& = (1-\\lambda) y_{i+1}^2 + \\lambda\\hat{\\sigma}^2_{i+1}\\\\\n&= (1-\\lambda) y_{i+1}^2 + \\lambda(1-\\lambda)  y_{i}^2 + \\lambda^2\\hat{\\sigma}^2_{i}\\;.\n\\end{align*}\nLikewise, the estimate at the following date will be\n$$\\hat{\\sigma}^2_{i+3} = (1-\\lambda) y_{i+2}^2 +\\lambda(1-\\lambda) y_{i+1}^2 + \\lambda^2(1-\\lambda)^2  y_{i}^2 +\\lambda^{3}\\hat{\\sigma}^2_{i}\\; .$$\nThis demonstrates the declining importance of the squared deviation $y_{i}^2$ for future estimates.  At each date, $y_{i}^2$ enters with a weight that is lower by a factor of $\\lambda$, compared to the previous date. \nIf $\\lambda$ is small, the decay in the importance of each squared deviation will be fast.  In fact, @eq-sig_estimator4 shows that, if $\\lambda$ is close to zero, the estimate $\\hat{\\sigma}_{i+1}^2$ is approximately equal to the squared deviation $y_i^2$---previous squared deviations are relatively unimportant.  On the other hand, if $\\lambda$ is close to one, the decay will be slow; i.e., the importance of $y_i^2$ for the estimate $\\hat{\\sigma}^2_{i+2}$ will be nearly the same as for $\\hat{\\sigma}^2_{i+1}$, and nearly the same for $\\hat{\\sigma}^2_{i+3}$ as for $\\hat{\\sigma}^2_{i+2}$, etc.   This will lead to a smooth (slowly varying) volatility estimate.  The slowly varying nature of the estimate in this case is also clear from @eq-sig_estimator4, because it shows that if $\\lambda$ is close to one, then $\\hat{\\sigma}^2_{i+1}$ will be approximately the same as $\\hat{\\sigma}^2_{i}$.\n\nThis method can also be used to estimate covariances, simply by replacing the squared deviations $y_i^2$ by the product of deviations for two different assets.  And, of course, given covariance and variance estimates, we can construct estimates of correlations.  To ensure that an estimated correlation is between $-1$ and $+1$, we will need to use the same $\\lambda$ to estimate each of the variances and the covariance.  This is the method used by RiskMetrics.^[See Mina and Xiao [@MX], available online at www.riskmetrics.com]. \\index{RiskMetrics}\n\n## GARCH Models {#sec-s_garch}\n\nWe are going to adopt a subtle but important change of perspective now.  Instead of considering @eq-sig_estimator4 as simply an estimation procedure, we are going to assume that the actual volatility evolves according to @eq-sig_estimator4, or a generalization thereof.  We are also going to reintroduce the expected change in $\\log S$, which we dropped in going from @eq-estimator_sig2 to @eq-estimator_sig2_3.  Specifically, we return to @eq-dlogs, but we operate under the risk-neutral measure, so \n$\\mu=r-q$, and we have\n$$\n\\log S(t_{i+1}) - \\log S(t_i) = \\left(r-q-\\frac{1}{2}\\sigma_{i+1}^2\\right)\\Delta t + \\sigma_{i+1} \\Delta B\\;.\n$$ {#eq-dlogs2}\n\nWe assume the volatility $\\sigma_{i+1}$ between dates $t_i$ and $t_{i+1}$ is given by\n$$\n\\sigma_{i+1}^2 = a + b y_{i}^2 + c \\sigma_i^2\\;,\n$$ {#eq-garch}\n\nfor some constants $a > 0$, $b\\geq 0$ and $c\\geq 0$, with $y_i$ now defined by\n$$y_i = \\frac{\\log S(t_i)-\\log S(t_{i-1})-\\left(r-q-\\frac{1}{2}\\sigma_i^2\\right)\\Delta t}{\\sqrt{\\Delta t}}\\; .$$\nFrom @eq-dlogs2, applied to the period from $t_{i-1}$ to $t_i$, this implies that $y_i$ is normally distributed with mean zero and variance $\\sigma_i^2$, and of course $y_{i+1}$ has variance $\\sigma_{i+1}^2$, etc.  \nUnder these assumptions, the random process $\\log S$ is called a \\index{GARCH process} GARCH(1,1) process.^[GARCH is the acronym for Generalized Autoregressive Conditional Heteroskedastic.  GARCH(1,1) means that there is only one past $y$  (no $y_{i-1}$, $y_{i-2}$, etc.) and one past $\\sigma$ (no  $\\sigma_{i-1}$, $\\sigma_{i-2}$, etc.) in  @eq-garch.  See Bollerslev [@Bollerslev].]  There are many varieties of GARCH processes that have been proposed in the literature, but we will only consider GARCH(1,1), which is the simplest.\n\nWe assume $b+c<1$, in which case we can write the variance equation as a generalization of @eq-sig_estimator4.  Namely,\n%$$\\sigma_{i+1}^2 = (1-\\phi)d + \\phi\\left[(1-\\lambda) y_{i}^2 + \\lambda \\sigma^2_{i}\\right]\\; ,$$\n$$\n\\sigma_{i+1}^2 = \\kappa\\theta + (1-\\kappa)\\left[  (1-\\lambda) y_{i}^2 + \\lambda\\sigma^2_{i}\\right]\\;,\n$$ {#eq-garch10}\n\nwhere $\\lambda=c/(b+c)$, \n%$\\phi=b+c$, and $d=a/(1-b-c)$.  \n$\\kappa = 1-b-c$, and $\\theta=a/(1-b-c)$.  Hence, $\\sigma_{i+1}^2$ is a weighted average with weights $\\kappa$ and $1-\\kappa$, of two parts, one being the constant $\\theta$ and the other being itself a weighted average of $y_{i}^2$ and $\\sigma^2_{i}$.  Whatever the variance might be at time $t_i$, the variance of $y_j$ at any date $t_j$ far into the future, computed without knowing the intervening $y_{i+1}, y_{i+2},\\ldots$, will be approximately the constant $\\theta$.  The constant $\\theta$ is called the unconditional variance, \\index{unconditional variance} whereas $\\sigma_{i}^2$ is the conditional variance of $y_i$.  \\index{conditional variance}\n\nTo understand the unconditional variance, it is useful to consider the variance forecasting equation.  Specifically, we can calculate $E_{t_i} \\left[\\sigma_{i+n}^2\\right]$, which is the estimate made at date $t_i$ of the variance of $y_{i+n}$; i.e, we estimate the variance without having observed $y_{i+1},\\ldots,y_{i+n-1}$.  Note that by definition $E_{t_{i}}[y_{i+1}^2]=\\sigma_{i+1}^2$, so @eq-garch10 implies\n\\begin{align*}\nE_{t_{i}}\\left[\\sigma_{i+2}^2\\right] &= \\kappa\\theta + (1-\\kappa)\\left[  (1-\\lambda) E_{t_{i}}[y_{i+1}^2] + \\lambda\\sigma^2_{i+1}\\right] \\\\\n&= \\kappa\\theta + (1-\\kappa)\\sigma^2_{i+1}\\; .\n\\end{align*}\nLikewise,\n$$E_{t_{i+1}}\\left[\\sigma_{i+3}^2\\right] = \\kappa\\theta + (1-\\kappa)\\sigma^2_{i+2}\\; ,$$\nand taking the expectation at date $t_i$ of both sides of this yields\n\\begin{align*}\nE_{t_{i}}\\left[\\sigma_{i+3}^2\\right] = E_{t_{i}}\\left[E_{t_{i+1}}\\left[\\sigma_{i+3}^2\\right]\\right] &=\\kappa\\theta + (1-\\kappa)E_{t_{i}}\\left[\\sigma_{i+2}^2\\right]\\\\\n&=\\kappa\\theta + (1-\\kappa)\\left[\\kappa\\theta + (1-\\kappa)\\sigma^2_{i+1}\\right]\\\\\n&=\\kappa\\theta[1+(1-\\kappa)] + (1-\\kappa)^2\\sigma^2_{i+1}\\;.\n\\end{align*}\nThis generalizes to\n$$E_{t_{i}}\\left[\\sigma_{i+n}^2\\right] = \\kappa\\theta\\left[1+(1-\\kappa)+ \\cdots (1-\\kappa)^{n-2}\\right] + (1-\\kappa)^{n-1}\\sigma^2_{i+1}\\; .$$\nThus, there is decay at rate $\\kappa$ in the importance of the current volatility $\\sigma^2_{i+1}$ for forecasting the future volatility.  Furthermore, as $n\\rightarrow \\infty$, the geometric series\n$$1+(1-\\kappa)+ \\cdots (1-\\kappa)^{n-2}$$\nconverges to $1/\\kappa$, so, as $n \\rightarrow \\infty$ we obtain\n$$E_{t_{i}}\\left[\\sigma_{i+n}^2\\right] \\rightarrow \\theta\\; .$$\nThis means that our best estimate of the conditional variance, at some date far in the future, is approximately the unconditional variance $\\theta$.\n\nThe most interesting feature of the volatility equation is that large returns (in absolute value) lead to an increase in the variance and hence are likely to be followed by more large returns (whether positive or negative).  This is the phenomenon of volatility clustering, \\index{volatility clustering} which is quite observable in actual markets.  This feature also implies that the distribution of returns will be fat tailed  (more technically, leptokurtic).  \\index{leptokurtic} This means that the probability of extreme returns is higher than under a normal distribution with the same standard deviation.^[Conversely, the probability of returns very near the mean must also be higher than under a normal distribution with the same standard deviation---a fat-tailed distribution must also have a relatively narrow peak.]  It is well documented that daily and weekly returns in most markets have this fat-tailed property.\n\nWe can simulate a path of an asset price that follows a GARCH process and the path of its volatility as follows.  The following python code produces three columns of data (with headings), the first column being time, the second  the asset price, and the third  the volatility.\n\n::: {#5af40abc .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\n\ndef simulating_garch(S, sigma, r, q, dt, N, theta, kappa, lambd):\n    \"\"\"\n    Inputs:\n    S = initial stock price\n    sigma = initial volatility\n    r = risk-free rate\n    q = dividend yield\n    dt = length of each time period (Delta t)\n    N = number of time periods\n    theta = theta parameter for GARCH\n    kappa = kappa parameter for GARCH\n    lambd = lambda parameter for GARCH\n    \"\"\"\n    LogS = np.log(S)\n    Sqrdt = np.sqrt(dt)\n    a = kappa * theta\n    b = (1 - kappa) * (1 - lambd)\n    c = (1 - kappa) * lambd\n    \n    time = np.zeros(N + 1)\n    stock_price = np.zeros(N + 1)\n    volatility = np.zeros(N + 1)\n    \n    stock_price[0] = S\n    volatility[0] = sigma    \n    \n    for i in range(1, N + 1):\n        time[i] = i * dt\n        y = sigma * np.random.randn()\n        LogS = LogS + (r - q - 0.5 * sigma * sigma) * dt + Sqrdt * y\n        S = np.exp(LogS)\n        stock_price[i] = S\n        sigma = np.sqrt(a + b * y ** 2 + c * sigma ** 2)\n        volatility[i] = sigma\n\n    df_garch = pd.DataFrame({'Time': time, 'Stock Price': stock_price, 'Volatility': volatility})\n    df_garch.to_csv('garch_simulation.csv', index=False)\n    return df_garch\n\n# Example usage:\nS = 100       # Initial stock price\nsigma = 0.2   # Initial volatility\nr = 0.05      # Risk-free rate\nq = 0.02      # Dividend yield\ndt = 1/252    # Length of each time period (daily)\nN = 252       # Number of time periods (one year)\ntheta = 0.1   # Theta parameter for GARCH\nkappa = 0.1   # Kappa parameter for GARCH\nlambd = 0.9   # Lambda parameter for GARCH\n\ndf_garch = simulating_garch(S, sigma, r, q, dt, N, theta, kappa, lambd)\nprint(df_garch)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Time  Stock Price  Volatility\n0    0.000000   100.000000    0.200000\n1    0.003968    99.974216    0.205917\n2    0.007937   100.168903    0.210780\n3    0.011905   101.372491    0.221824\n4    0.015873    99.177157    0.246474\n..        ...          ...         ...\n248  0.984127   105.972118    0.330689\n249  0.988095   109.522062    0.351209\n250  0.992063   111.088665    0.338479\n251  0.996032   111.151724    0.320641\n252  1.000000   113.163568    0.317244\n\n[253 rows x 3 columns]\n```\n:::\n:::\n\n\nTo price European options, we need to compute the usual probabilities \n$\\text{prob}^S(S(T)>K)$ and $\\text{prob}^R(S(T) >K)$.\nHeston and Nandi [@HN] provide a fast method for computing these probabilities in a GARCH (1,1) model.^[Actually, a slightly more general model is considered in [@HN], in which large negative returns lead to a greater increase in volatility than do large positive returns.  This accommodates the empirically observed negative correlation between stock returns and volatility.]  Rather than developing this approach, we will show in @sec-c_introcomputation how to apply Monte-Carlo methods.\n\n\n\n## Stochastic Volatility Models {#sec-s_stochasticvolatility}\n\nThe volatility is stochastic (random) in a GARCH model, but it is determined by the changes in the stock price.  In this section, in contrast, we will consider models in which the volatility depends on a second Brownian motion.  \\index{stochastic volatility} The most popular model of this type is the model of Heston [@Heston].  \\index{Heston model} In this model, we have, as usual, \n\n\n$$\n\\mathrm{d}\\log S = \\left(r-q-\\frac{1}{2}\\sigma^2\\right)\\,\\mathrm{d} t + \\sigma\\,\\mathrm{d} B_s\\;,\n$$ {#eq-heston1}\n\nwhere $B_s$ is a Brownian motion under the risk-neutral measure but now $\\sigma$ is not a constant but instead evolves as $\\sigma(t) = \\sqrt{v(t)}$ where\n\n$$\ndv(t) = \\kappa \\big[\\theta-v(t)\\big]\\,\\mathrm{d} t + \\gamma \\sqrt{v(t)}\\,\\mathrm{d} B_v\\;,\n$$ {#eq-heston2}\n\n\nwhere $B_v$ is a Brownian motion under the risk-neutral measure having a constant correlation $\\rho$ with the Brownian motion $B_s$.  In this equation, $\\kappa$, $\\theta$ and $\\gamma$ are positive constants.  Given the empirical fact that negative return shocks have a bigger impact on future volatility than do positive shocks, one would expect the correlation $\\rho$ to be negative.\n\nThe term $\\kappa (\\theta-v)$ will be positive when $v<\\theta$ and negative when $v>\\theta$ and hence $\\sigma^2=v$ will tend to drift towards $\\theta$, which, as in the GARCH model, is the long-run or unconditional mean of $\\sigma^2$.  Thus, the volatility is said to mean revert. \\index{mean reversion} The rate at which it drifts towards $\\theta$ is obviously determined by the magnitude of $\\kappa$, also as in the GARCH model.  \n\nThe specification @eq-heston2 implies that the volatility of $v$ approaches zero whenever $v$ approaches zero.      In this circumstance, one might expect the drift towards $\\theta$ to dominate the volatility and keep $v$ nonnegative, and this is indeed the case; thus, the definition $\\sigma(t) = \\sqrt{v(t)}$ is possible.  Moreover, the parameter $\\gamma$ plays a role here that is similar to the role of $1-\\lambda$ in the GARCH model---the variance of the variance in the GARCH model @eq-garch10 depends on the weight $1-\\lambda$ placed on the scaled return $y_i$, just as the variance of the variance in the stochastic volatility model @eq-heston2 depends on the weight $\\gamma$ placed on  $\\mathrm{d} B_v$.\n\nWe could discretize @eq-heston1 - @eq-heston2 as:\n\n\n$$\n\\log S(t_{i+1}) = \\log S(t_i) + \\left(r-q-\\frac{1}{2}\\sigma(t_i)^2\\right)\\,\\Delta t + \\sqrt{v(t_i)}\\,\\Delta B_s,\n$$ {#eq-heston3}\n\n$$\nv(t_{i+1}) = v(t_i) + \\kappa \\big[\\theta-v(t_i)\\big]\\,\\Delta t + \\gamma \\sqrt{v(t_i)}\\,\\Delta B_v\\;.\n$$ {#eq-heston4}\n\n\n\nHowever, even though in the continuous-time model @eq-heston1 - @eq-heston2 we always have $v(t) \\geq 0$ and hence can define $\\sigma(t)=\\sqrt{v(t)}$, there is no guarantee that $v(t_{i+1})$ defined by @eq-heston4 will be nonnegative.  A simple remedy is to define $v(t_{i+1})$ as the larger of zero and the right-hand side of @eq-heston4; thus, we will simulate the Heston model as @eq-heston3 and^[There are better (but more complicated) ways to simulate the Heston model.  An excellent discussion of ways to simulate the volatility process can be found in Glasserman [@Glasserman].  Broadie and Kaya [@broadiekaya] present a method for simulating from the exact distribution of the asset price in the Heston model and related models.]\n$$\nv(t_{i+1}) = \\max\\left\\{0,v(t_i) + \\kappa \\big[\\theta-v(t_i)\\big]\\,\\Delta t + \\gamma \\sqrt{v(t_i)}\\,\\Delta B_v\\right\\}\\;.\n$$ {#eq-heston41}\n\n\n\nA simple way to simulate the changes $\\Delta B_s$ and $\\Delta B_v$ in the two correlated Brownian motions is to generate two independent standard normals $z_1$ and $z_2$ and take\n$$\\Delta B_s = \\sqrt{\\Delta t}\\,z \\qquad \\text{and} \\qquad \\Delta B_v = \\sqrt{\\Delta t}\\,z^*\\; ,$$\nwhere we define \n$$z = z_1 \\qquad \\text{and} \\qquad z^* = \\rho z_1 + \\sqrt{1-\\rho^2}\\,z_2\\; .$$\nThe random variable $z^*$ is also a standard normal, and the correlation between $z$ and $z^*$ is $\\rho$. \n\nAs in the GARCH model, we can simulate a path of an asset price that follows a GARCH process and the path of its volatility as follows.  The following python code produces three columns of data (with headings), the first column being time, the second  the asset price, and the third  the volatility in the Heston model.\n\n::: {#566a8f74 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\n\ndef simulating_stochastic_volatility(S, V0, r, q, dt, N, theta, kappa, sigma, rho):\n    \"\"\"\n    Inputs:\n    S = initial stock price\n    V0 = initial variance\n    r = risk-free rate\n    q = dividend yield\n    dt = length of each time period (Delta t)\n    N = number of time periods\n    theta = long-term variance (mean of variance)\n    kappa = rate of mean reversion of variance\n    sigma = volatility of variance\n    rho = correlation between the two Wiener processes\n    \"\"\"\n    LogS = np.log(S)\n    Sqrdt = np.sqrt(dt)\n    \n    time = np.zeros(N + 1)\n    stock_price = np.zeros(N + 1)\n    volatility = np.zeros(N + 1)\n    \n    stock_price[0] = S\n    volatility[0] = V0    \n    \n    for i in range(1, N + 1):\n        time[i] = i * dt\n        Z1 = np.random.randn()\n        Z2 = np.random.randn()\n        W1 = Z1\n        W2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n        \n        LogS = LogS + (r - q - 0.5 * volatility[i-1]**2) * dt + np.sqrt(volatility[i-1]**2 * dt) * W1\n        S = np.exp(LogS)\n        stock_price[i] = S\n        \n        volatility[i] = np.sqrt(np.maximum(volatility[i-1]**2 + kappa * (theta - volatility[i-1]**2) * dt + sigma * np.sqrt(volatility[i-1]**2 * dt) * W2, 0))\n\n    df_stochastic_vol = pd.DataFrame({'Time': time, 'Stock Price': stock_price, 'Volatility': volatility})\n    df_stochastic_vol.to_csv('stochastic_volatility_simulation.csv', index=False)\n    return df_stochastic_vol\n\n# Example usage:\nS = 100       # Initial stock price\nV0 = 0.04     # Initial variance\nr = 0.05      # Risk-free rate\nq = 0.02      # Dividend yield\ndt = 1/252    # Length of each time period (daily)\nN = 252       # Number of time periods (one year)\ntheta = 0.04  # Long-term variance\nkappa = 2.0   # Rate of mean reversion of variance\nsigma = 0.3   # Volatility of variance\nrho = -0.7    # Correlation between the two Wiener processes\n\ndf_stochastic_vol = simulating_stochastic_volatility(S, V0, r, q, dt, N, theta, kappa, sigma, rho)\nprint(df_stochastic_vol)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Time  Stock Price  Volatility\n0    0.000000   100.000000    0.040000\n1    0.003968   100.077651    0.048325\n2    0.007937    99.692417    0.061462\n3    0.011905    99.441478    0.066690\n4    0.015873    99.155049    0.075899\n..        ...          ...         ...\n248  0.984127   113.072412    0.195979\n249  0.988095   114.414897    0.194887\n250  0.992063   114.857972    0.181152\n251  0.996032   113.783101    0.187392\n252  1.000000   114.582149    0.177125\n\n[253 rows x 3 columns]\n```\n:::\n:::\n\n\nThe following code plots the simulated stock price and the variance paths. \n\n::: {#d983040e .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Heston model parameters\nS0 = 100     # Initial stock price\nV0 = 0.04    # Initial variance\nrho = -0.7   # Correlation between the two Wiener processes\nkappa = 2.0  # Rate of mean reversion of variance\ntheta = 0.04 # Long-term variance\nsigma = 0.3  # Volatility of variance\nr = 0.05     # Risk-free rate\nT = 1.0      # Time to maturity\nN = 252      # Number of time steps\ndt = T / N   # Time step size\nn_simulations = 1000  # Number of simulations\n\n# Preallocate arrays\nS = np.zeros((N+1, n_simulations))\nV = np.zeros((N+1, n_simulations))\nS[0] = S0\nV[0] = V0\n\n# Simulate the paths\nfor t in range(1, N+1):\n    Z1 = np.random.normal(size=n_simulations)\n    Z2 = np.random.normal(size=n_simulations)\n    W1 = Z1\n    W2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n    \n    V[t] = np.maximum(V[t-1] + kappa * (theta - V[t-1]) * dt + sigma * np.sqrt(V[t-1] * dt) * W2, 0)\n    S[t] = S[t-1] * np.exp((r - 0.5 * V[t-1]) * dt + np.sqrt(V[t-1] * dt) * W1)\n\n# Plot the results\nplt.figure(figsize=(12, 6))\nfor i in range(n_simulations):\n    plt.plot(S[:, i], lw=0.5, alpha=0.3)\nplt.title('Stock Price Paths with Stochastic Volatility (Heston Model)')\nplt.xlabel('Time Steps')\nplt.ylabel('Stock Price')\nplt.grid(True)\nplt.show()\n\n# Plot the volatility paths\nplt.figure(figsize=(12, 6))\nfor i in range(n_simulations):\n    plt.plot(V[:, i], lw=0.5, alpha=0.3)\nplt.title('Variance Paths with Stochastic Volatility (Heston Model)')\nplt.xlabel('Time Steps')\nplt.ylabel('Variance')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Chapter5_files/figure-html/cell-4-output-1.png){width=965 height=523}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Chapter5_files/figure-html/cell-4-output-2.png){width=969 height=523}\n:::\n:::\n\n\nThe following code shows that the stock returns under the stochastic volatility model display fat-tails with positive kurtosis. \n\n::: {#c16cddcd .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm, kurtosis\n\ndef simulating_stochastic_volatility(S, V0, r, q, dt, N, theta, kappa, sigma, rho, n_simulations):\n    \"\"\"\n    Inputs:\n    S = initial stock price\n    V0 = initial variance\n    r = risk-free rate\n    q = dividend yield\n    dt = length of each time period (Delta t)\n    N = number of time periods\n    theta = long-term variance (mean of variance)\n    kappa = rate of mean reversion of variance\n    sigma = volatility of variance\n    rho = correlation between the two Wiener processes\n    n_simulations = number of simulations\n    \"\"\"\n    LogS = np.log(S)\n    Sqrdt = np.sqrt(dt)\n    \n    log_returns = []\n    \n    for _ in range(n_simulations):\n        stock_price = S\n        variance = V0\n        for _ in range(N):\n            Z1 = np.random.randn()\n            Z2 = np.random.randn()\n            W1 = Z1\n            W2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n            \n            log_return = (r - q - 0.5 * variance) * dt + np.sqrt(variance * dt) * W1\n            LogS = np.log(stock_price) + log_return\n            stock_price = np.exp(LogS)\n            \n            variance = np.maximum(variance + kappa * (theta - variance) * dt + sigma * np.sqrt(variance * dt) * W2, 0)\n            \n            log_returns.append(log_return)\n    \n    return log_returns\n\n# Example usage:\nS = 100       # Initial stock price\nV0 = 0.04     # Initial variance\nr = 0.05      # Risk-free rate\nq = 0.02      # Dividend yield\ndt = 1/252    # Length of each time period (daily)\nN = 252       # Number of time periods (one year)\ntheta = 0.04  # Long-term variance\nkappa = 0.2   # Rate of mean reversion of variance\nsigma = 0.3   # Volatility of variance\nrho = -0.7    # Correlation between the two Wiener processes\nn_simulations = 1000  # Number of simulations\n\nlog_returns = simulating_stochastic_volatility(S, V0, r, q, dt, N, theta, kappa, sigma, rho, n_simulations)\n\n# Plotting the distribution of log returns\nsns.histplot(log_returns, bins=100, kde=True, stat=\"density\", color=\"blue\", label=\"Simulated Log Returns\")\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, np.mean(log_returns), np.std(log_returns))\nplt.plot(x, p, 'k', linewidth=2, label=\"Normal Distribution\")\nplt.title('Distribution of Log Returns with Stochastic Volatility')\nplt.xlabel('Log Return')\nplt.ylabel('Density')\nplt.legend()\n\n# Display kurtosis\nkurt = kurtosis(log_returns)\nplt.figtext(0.15, 0.8, f'Kurtosis: {kurt:.2f}', fontsize=12)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Chapter5_files/figure-html/cell-5-output-1.png){width=585 height=449}\n:::\n:::\n\n\nTo price European options, we again need to compute \n$$\\text{prob}^S(S(T)>K) \\qquad \\text{and} \\qquad \\text{prob}^R(S(T)>K)\\; .$$\nThe virtue of modelling volatility as in @eq-heston2 is that these probabilities can be computed quite efficiently, as shown by Heston [@Heston].^[Further discussion can be found in Epps [@Epps].]  There are many other ways in which one could model volatility, but the computations may be more difficult.  For example, one could  replace @eq-heston2 by\n$$\n\\sigma(t) = \\mathrm{e}^{v(t)} \\quad \\text{and} \\quad dv(t) = \\kappa (\\theta-v(t))\\,\\mathrm{d} t + \\lambda \\,\\mathrm{d} B^*\\;.\n$$ {#eq-heston5}\n\nThis implies a lognormal volatility and is simpler to simulate than @eq-heston2---because $\\mathrm{e}^{v}$ is well defined even when $v$ is negative---but it is easier to calculate the probabilities $\\text{prob}^S(S(T)>K)$ and $\\text{prob}^R(S(T)>K)$ if we assume @eq-heston2.\n\nOne way to implement the GARCH or stochastic volatility model is to imply both the initial volatility $\\sigma(0)$ and the constants $\\kappa$, $\\theta$ and $\\lambda$ or $\\kappa$, $\\theta$, $\\gamma$ and $\\rho$ from observed option prices.  These four (or five) constants can be computed by forcing the model prices of four (or five) options to equal the observed market prices.  Or, a larger set of prices can be used and the constants can be chosen to minimize the average squared error or some other measure of goodness-of-fit between the model and market prices.\n\n## Jump Diffusion Models {#sec-s_jumpdiffusion}\n\nIn the classical Black-Scholes model, stock prices are assumed to follow a geometric Brownian motion, which is characterized by continuous paths and normally distributed returns. While this model has been widely used due to its simplicity and analytical tractability, it fails to capture certain empirical phenomena observed in financial markets, such as sudden and significant price changes (jumps) and the heavy tails of return distributions.\n\nTo address these shortcomings, the jump diffusion model was introduced by Robert C. Merton in 1976. This model extends the Black-Scholes framework by incorporating jumps into the stock price dynamics, thereby allowing for discontinuous price paths. The jump diffusion model is better suited to describe the behavior of financial assets that exhibit sudden price changes due to news, earnings announcements, or other market events.\n\nIn a jump diffusion model, the stock price $S_t$ is governed by the following stochastic differential equation (SDE):\n\n$$\ndS_t = \\mu S_t \\, dt + \\sigma S_t \\, dW_t + S_t \\, dJ_t\n$$\n\nwhere $\\mu$ is the drift rate, $\\sigma$ is the volatility, $B_t$ is a standard Brownian motion, $J_t$ is a jump process.\n\nThe jump process $J_t$ is typically modeled as a compound Poisson process:\n\n$$\nJ_t = \\sum_{i=1}^{N_t} (Y_i - 1)\n$$\n\nwhere $N_t$ is a Poisson process with intensity $\\lambda$, $Y_i$ are i.i.d. random variables representing the relative jump sizes, with $Y_i - 1$ being the actual jump size.\n\nThe jump diffusion model introduces several important implications for the behavior of stock prices and the pricing of derivative securities:\n\n1. **Heavy Tails**: The inclusion of jumps leads to a return distribution with heavier tails compared to the normal distribution, aligning better with empirical observations.\n2. **Volatility Smile**: The model can generate implied volatility smiles, where implied volatility varies with strike price and maturity, a feature commonly observed in market data.\n3. **Risk Management**: Understanding the jump component is crucial for risk management, as it affects the likelihood of extreme price movements and the potential for large losses.\n\nThe following python code simulates the stock price that evolves as a jump diffusion. \n\n::: {#cell-Simulate_Jump_Diffusion .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_jump_diffusion(S0, mu, sigma, lamb, m, delta, T, N):\n    \"\"\"\n    Simulate a jump diffusion process.\n    \n    Parameters:\n    S0     : float - initial stock price\n    mu     : float - drift rate\n    sigma  : float - volatility\n    lamb   : float - intensity of the Poisson process\n    m      : float - mean of the jump size distribution\n    delta  : float - standard deviation of the jump size distribution\n    T      : float - total time\n    N      : int   - number of time steps\n    \n    Returns:\n    t      : numpy array - time points\n    S      : numpy array - simulated stock prices\n    \"\"\"\n    dt = T / N\n    t = np.linspace(0, T, N + 1)\n    S = np.zeros(N + 1)\n    S[0] = S0\n    \n    for i in range(1, N + 1):\n        Z = np.random.normal(0, 1)  # Normal random variable for the diffusion part\n        J = np.random.poisson(lamb * dt)  # Poisson random variable for jumps\n        \n        # Sum of log-normal distributed jumps\n        Y = np.sum(np.random.normal(m, delta, J))\n        \n        # Update stock price\n        S[i] = S[i - 1] * np.exp((mu - 0.5 * sigma ** 2) * dt + sigma * np.sqrt(dt) * Z + Y)\n    \n    return t, S\n\n# Parameters\nS0 = 100         # Initial stock price\nmu = 0.1         # Drift rate\nsigma = 0.2      # Volatility\nlamb = 0.75      # Intensity of the Poisson process (average number of jumps per unit time)\nm = 0.02         # Mean of the jump size distribution (log-normal)\ndelta = 0.1      # Standard deviation of the jump size distribution (log-normal)\nT = 1.0          # Total time (1 year)\nN = 1000         # Number of time steps\n\n# Simulate the jump diffusion process\nt, S = simulate_jump_diffusion(S0, mu, sigma, lamb, m, delta, T, N)\n\n# Plot the simulated stock prices\nplt.figure(figsize=(10, 6))\nplt.plot(t, S, label='Jump Diffusion Process')\nplt.title('Simulated Stock Price using Jump Diffusion Model')\nplt.xlabel('Time (years)')\nplt.ylabel('Stock Price')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Chapter5_files/figure-html/simulate_jump_diffusion-output-1.png){#simulate_jump_diffusion width=816 height=523}\n:::\n:::\n\n\nThe jump diffusion model offers a more realistic framework for modeling stock prices by incorporating the possibility of sudden jumps. This enhancement over the classical Black-Scholes model allows for better capturing the empirical characteristics of financial markets, thereby improving the accuracy of option pricing and risk management practices.\n\n\n## Smiles and Smirks Again {#sec-s_smilesagain}\n\nAs mentioned before, the GARCH, stochastic volatility and jump diffusion models can generate  fat-tailed distributions for the asset price $S(T)$.  Thus, they can be more nearly consistent with the option smiles \\index{smile}discussed in @sec-s_smiles than is the Black-Scholes model (though it appears that one must include jumps in asset prices as well as stochastic volatility in order to duplicate market prices with an option pricing formula).  To understand the relation, let $\\sigma_\\text{am}$ denote the implied volatility from an at-the-money call option, i.e., a call option with strike $K=S(0)$.  The characteristic of a smile is that implied volatilities from options of the same maturity with strike prices significantly above and below $S(0)$ are higher than $\\sigma_\\text{am}$.  \n\nA strike price higher than $S(0)$  corresponds to an out-of-the money call option.  The high implied volatility means that the market is pricing the right to buy at $K>S(0)$ above the Black-Scholes price computed from the volatility $\\sigma_\\text{am}$; thus, the market must attach a higher probability to stock prices $S(T)>S(0)$ than the  volatility $\\sigma_\\text{am}$ would suggest.  \n\nA strike price lower than $S(0)$ corresponds to an in-the-money call option.  The put option with the same strike is out of the money.  The high implied volatility means that the market is pricing call options  above the Black-Scholes price computed from the volatility $\\sigma_\\text{am}$.  By put-call parity, the market must also be pricing put options above the Black-Scholes price computed from the  volatility $\\sigma_\\text{am}$.  The high prices for the rights to buy and sell at $K<S(0)$ means that the market must attach a higher probability to stock prices $S(T)<S(0)$ than the volatility $\\sigma_\\text{am}$ would suggest.  In particular, the high price for the right to sell at $K<S(0)$ means a high insurance premium for owners of the asset who seek to insure their positions, which is consistent with a market view that there is a significant probability of a large loss.  This can be interpreted as a crash premium.  \\index{crash premium} Indeed, the implied volatilities at strikes less than $S(0)$  are typically higher than the implied volatilities at strikes above $S(0)$ (giving the smile the appearance of a smirk, as discussed in @sec-s_smiles), which is consistent with a larger probability of crashes than of booms (a fatter tail for low returns than for high).\n\nAs an example, the following code shows that the stochstic volatility model can generate implied volatility smiles. \n\nThis program involes (1) Simulating Heston Model: simulate_heston_paths function generates stock price paths using the Heston model parameters. (2) Calculating call price by discounting the averaged call payoffs across the stock price sample paths (3) Calculating Black Scholes call price : black_scholes_call_price function calculates the call option price using the Black-Scholes formula.\n(4) Calculating implied volatility: implied_volatility function computes the implied volatility by solving for the volatility that matches the Black-Scholes call price to the simulated call price. (5) Repeating Steps (2)-(4) for different strik prices. (6) Plotting the implied volatility against strike prices fixing the initial stock price at \\$100.   \n\n::: {#21256c78 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom scipy.optimize import brentq\n\n# Heston model parameters\nS0 = 100      # Initial stock price\nV0 = 0.04     # Initial variance\nr = 0.05      # Risk-free rate\nq = 0.01      # Dividend yield\nT = 1         # Time to maturity (in years)\nkappa = 0.25   # Rate of mean reversion of variance\ntheta = 0.04  # Long-term variance\nsigma = 0.5   # Volatility of variance\nrho = -0.2    # Correlation between the two Wiener processes\ndt = 1/252    # Length of each time period (daily)\nN = 252       # Number of time periods (one year)\nn_simulations = 10000  # Number of simulations\n\ndef simulate_heston_paths(S0, V0, r, q, T, kappa, theta, sigma, rho, dt, N, n_simulations):\n    S = np.zeros((N + 1, n_simulations))\n    V = np.zeros((N + 1, n_simulations))\n    S[0] = S0\n    V[0] = V0\n    \n    for t in range(1, N + 1):\n        Z1 = np.random.normal(size=n_simulations)\n        Z2 = np.random.normal(size=n_simulations)\n        W1 = Z1\n        W2 = rho * Z1 + np.sqrt(1 - rho**2) * Z2\n        \n        V[t] = np.maximum(V[t-1] + kappa * (theta - V[t-1]) * dt + sigma * np.sqrt(V[t-1] * dt) * W2, 0)\n        S[t] = S[t-1] * np.exp((r - q - 0.5 * V[t-1]) * dt + np.sqrt(V[t-1] * dt) * W1)\n    \n    return S, V\n\ndef black_scholes_call_price(S, K, T, r, sigma):\n    d1 = (np.log(S / K) + (r + 0.5 * sigma**2) * T) / (sigma * np.sqrt(T))\n    d2 = d1 - sigma * np.sqrt(T)\n    call_price = S * norm.cdf(d1) - K * np.exp(-r * T) * norm.cdf(d2)\n    return call_price\n\ndef implied_volatility(C, S, K, T, r):\n    def objective_function(sigma):\n        return black_scholes_call_price(S, K, T, r, sigma) - C\n    try:\n        return brentq(objective_function, 0.001, 5.0)\n    except ValueError:\n        return np.nan\n\n# Simulate paths using the Heston model\nS_paths, _ = simulate_heston_paths(S0, V0, r, q, T, kappa, theta, sigma, rho, dt, N, n_simulations)\n\n# Calculate option prices at different strike prices\nstrike_prices = np.linspace(90, 120, 20)\ncall_prices = np.zeros_like(strike_prices)\n\nfor i, K in enumerate(strike_prices):\n    call_payoffs = np.maximum(S_paths[-1] - K, 0)\n    call_prices[i] = np.mean(call_payoffs) * np.exp(-r * T)\n\n# Calculate implied volatilities\nimplied_vols = [implied_volatility(C, S0, K, T, r) for C, K in zip(call_prices, strike_prices)]\n\n# Filter out NaN values that may occur\nvalid_indices = ~np.isnan(implied_vols)\nstrike_prices = strike_prices[valid_indices]\nimplied_vols = np.array(implied_vols)[valid_indices]\n\n# Plot the implied volatility smile\nplt.figure(figsize=(10, 6))\nplt.plot(strike_prices, implied_vols, label='Implied Volatility', marker='o')\nplt.title('Implied Volatility Smile')\nplt.xlabel('Strike Price')\nplt.ylabel('Implied Volatility')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Chapter5_files/figure-html/cell-7-output-1.png){width=829 height=523}\n:::\n:::\n\n\n## Hedging and Market Completeness\n\nThe GARCH model is inherently a discrete-time model.  If returns have a GARCH structure at one frequency (e.g., monthly), they will not have a GARCH structure at a different frequency (e.g., weekly).  Hence, the return period (monthly, weekly, \\ldots) is part of the specification of the model.  One interpretation of the model is that the dates $t_i$ at which the variance changes are the only dates at which investors can trade.  Under this interpretation, it is impossible to perfectly hedge an option: the gross return $S(t_i)/S(t_{i-1})$ over the interval $(t_{i-1},t_i)$ is lognormally distributed, so no portfolio of the stock and riskless asset formed at $t_{i-1}$ and held over the interval $(t_{i-1},t_i)$ can perfectly replicate the return of an option over the interval.  As discussed in @sec-s_incomplete, we call a market in which some derivatives cannot be perfectly hedged an incomplete market. \\index{incomplete market}  Thus, the GARCH model is an example of an incomplete market, if investors can only trade at the frequency at which returns have a GARCH structure.  However, it is unreasonable to assume that investors can only trade weekly or monthly or even daily.\n\nAnother interpretation of the GARCH model is that investors can trade continuously and the asset has a constant volatility within each period $(t_{i-1},t_i)$.  Under this interpretation, the market is complete and options can be delta-hedged.  The completeness is a result of the fact that the change $\\sigma_{i+1}-\\sigma_i$ in the volatility  at date $t_i$ (recall that $\\sigma_i$ is the volatility over the period $(t_{i-1},t_i)$ and $\\sigma_{i+1}$ is the volatility over the period $(t_{i},t_{i+1})$) depends only on $\\log S(t_i)$.  Thus, the only random factor in the model that needs to be hedged is, as usual, the underlying asset price.  However, this interpretation of the model is also a bit strange.  Suppose for example that monthly returns are assumed to have a GARCH structure.  Then the model states that the volatility in February will be higher if there is an unusually large return (in absolute value) in January.  Suppose there is an unusually large return in the first half of January.  Then, intuitively, one would expect the change in the volatility to occur in the second half of January rather than being delayed until February.  However, the model specifies that the volatility is constant during each month, hence constant during January in this example.\n\nThe stochastic volatility model is more straightforward.  The market is definitely incomplete.  The value of a call option at date $t<T$, where $T$ is the maturity of the option, will depend on the underlying asset price $S(t)$ and the volatility $\\sigma(t)$.  Denoting the value by $C(t,S(t),\\sigma(t))$, we have\nfrom Ito's formula that\n$$\n\\mathrm{d} C(t) = \\text{something}\\;\\mathrm{d} t + \\frac{\\partial C}{\\partial S}\\,\\mathrm{d} S(t) + \\frac{\\partial C}{\\partial \\sigma}\\,\\mathrm{d} \\sigma(t)\\; .$$\nA replicating portfolio must have the same dollar change at each date $t$.  If we hold $\\partial C/\\partial S$ shares of the underlying asset, then the change in the value of the shares will be $(\\partial C/\\partial S)\\,\\mathrm{d} S$.  However, there is no way to match the  \n$(\\partial C/\\partial \\sigma)\\,\\mathrm{d} \\sigma$ term using the underlying asset and the riskless asset.  \n\nThe significance of the market being incomplete is that the value of a derivative asset that cannot be replicated using traded assets (e.g., the underlying and riskless assets) is not uniquely determined by arbitrage considerations.  As discussed in @sec-s_incomplete, one must use equilibrium pricing in this circumstance.  \\index{equilibrium pricing} That is what we have implicitly done in this chapter.  By assuming particular dynamics for the volatility under the risk-neutral measure, we have implicitly selected a particular risk-neutral measure from the set of risk-neutral measures that are consistent with the absence of arbitrage.  \n\n\n## Exercises\n\n::: {#exr-e_mixture}\n  The purpose of this exercise is to generate a fat-tailed distribution from a model that is simpler than the GARCH and stochastic volatility models but has somewhat the same flavor.  The distribution will be a mixture of normals. Create a python program in which the user can input $S$, $r$, $q$, $T$, $\\sigma_1$ and $\\sigma_2$.  Use these inputs to produce a column of 500 simulated $\\log S(T)$.  In each simulation, define $\\log S(T)$ as\n$$\\log S(T) = \\log S(0) + \\left(r-q-\\frac{1}{2}\\sigma^2\\right)T + \\sigma \\sqrt{T}z\\;,$$\nwhere $z$ is a standard normal,\n$\\sigma = x\\sigma_1 + (1-x)\\sigma_2$,\nand $x$ is a random variable that equals zero or one with equal probabilities.  \n\nCalculate the mean and standard deviation of the $\\log S(T)$ and calculate the fraction that lie more than two standard deviations below the mean.  If the $\\log S(T)$ all came from a normal distribution with the same variance, then this fraction should equal $\\mathrm{N}(-2) =$ 2.275\\%.  If the fraction is higher, then the distribution is fat tailed.  (Of course, the actual fraction would differ from 2.275\\% in any particular case due to the randomness of the simulation, even if all of the $\\log S(T)$ came from a normal distribution with the same variance).\n:::\n\n::: {#exr-e_GARCH1}\n  Create a python program prompting the user to input the same inputs as in the `simulating_garch` function except for the initial volatility and $\\theta$.  Simulate 500 paths of a GARCH process and output $\\log S(T)$ for each simulation (you don't need to output the entire paths as in the `simulating_garch` function).  Take the initial volatility to be 0.3 and $\\theta = 0.09$.  Determine whether the distribution is fat-tailed by computing the fraction of the $\\log S(T)$ that lie two or more standard deviations below the mean, as in the previous exercise.  For what values of $\\kappa$ and $\\lambda$ does the distribution appear to be especially fat-tailed? \n:::\n::: {#exr-nolabel}\n Repeat @exr-e_GARCH1 for the Heston stochastic volatility model, describing the values of $\\kappa$,  $\\gamma$ and $\\rho$ that appear to generate especially fat-tailed distributions.\n\n:::\n\n",
    "supporting": [
      "Chapter5_files"
    ],
    "filters": [],
    "includes": {}
  }
}