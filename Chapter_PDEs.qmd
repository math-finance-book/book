{{< include macros.qmd >}}

# PDE Methods {#sec-c:pde} 

```{python}
#| eval: true
#| echo: false

import plotly
from IPython.display import display, HTML

plotly.offline.init_notebook_mode(connected=True)
display(
    HTML(
        '<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"></script>'
    )
)
```


In this chapter we will see how to estimate derivative values by numerically solving the partial differential equation (pde) that the derivative value satisfies, using finite difference methods.  More advanced discussions of this topic can be found in Wilmott, DeWynne and Howison [@WDH], Wilmott [@Wilmott], and Tavella [@Tavella],  among other places.   We will only consider derivatives written on a single underlying asset, but the ideas generalize to derivatives written on multiple underlying assets (e.g., basket and spread options) in much the same way that binomial models can be applied to derivatives on multiple underlying assets.  The curse of dimensionality is the same for finite difference methods as for binomial models---the computation time increases exponentially with the number of underlying assets.

## Fundamental PD\\E {#sec-fundamentalpde}

\index{fundamental pde}Consider an asset with price $S$ and constant dividend yield $q$.  
Set $X=\log S$.  Then we have
$$\d  X = \nu\d   t+\sigma\d   B\; ,$$
where $\nu =r-q-\sigma^2/2$ and $B$ is a Brownian motion under the risk-neutral probability.

Let $T$ denote the maturity date of a derivative security.  At time $t$ (when the remaining time to maturity is $T-t$), assume the price of the derivative can be represented as $C(t,X_t)$.^[If the price of the derivative is a function of the asset price $S$ and time, then we can always write it in this form as a function of the natural logarithm of $S$ and time.]  
Since $C$ is a function of $t$ and $X$, Ito's formula implies

$$
\d  C  = \frac{\partial C}{\partial t}\d   t + \frac{\partial C}{\partial X}\d   X +\frac{1}{2}\frac{\partial^2 C}{\partial X^2}(\d  X)^2
$$ {#eq-2}

$$
=\frac{\partial C}{\partial t}\d   t+ \frac{\partial C}{\partial X}\big(\nu\d   t+\sigma\d   B\big) + \frac{1}{2}\frac{\partial^2 C}{\partial X^2}\sigma^2\d   t\;.
$$

On the other hand, under the risk-neutral probability, the instantaneous expected rate of return on the derivative is the risk-free rate, so
$$\frac{\d  C}{C} =r\d   t + \text{something}\d    B\; .$$
where the something is the volatility of the derivative value.  We can of course rearrange this as
$$
\d  C = rC\d   t+\text{something}\,\,C\d   B\;.
$$ {#eq-1}

In order for both @eq-2 and @eq-1 to hold, the drifts on both right-hand sides must be equal.^[Suppose a process $X$ satisfies $\d  X=\alpha_1\d   t+\sigma_1\d   B = \alpha_2\d   t+\sigma_2\d   B$ for coefficients $\alpha_i$ and $\sigma_i$.  This implies $(\alpha_1-\alpha_2)\d   t=(\sigma_2-\sigma_2)\d   B$.  The right-hand side defines a (local) martingale and the left-hand side defines a continuous finite-variation process.  As discussed in @sec-s:quadraticvariation, the only continuous finite-variation martingales are constants, so the changes must be zero; i.e., $\alpha_1=\alpha_2$ and $\sigma_1=\sigma_2$.] This implies
$$
rC = \frac{\partial C}{\partial t}+ \nu\frac{\partial C}{\partial X}+ \frac{1}{2}\sigma^2\frac{\partial^2 C}{\partial X^2}\;.
$$ {#eq-3}

This equation is the fundamental pde.  It is an equation that we want to solve for the function $C$.  Every derivative written on $S$ satisfies this same equation.  Different derivatives have different values because of boundary conditions.  The boundary conditions are the intrinsic value at maturity, optimality conditions for early exercise, barriers and the like.

To translate the terms in @eq-3 into more familiar ones, notice that, because $S=\mathrm{e}^X$, we have
$$
\frac{\partial S}{\partial X}=\mathrm{e}^X=S\;.
$$  Therefore, by the chain rule of calculus,
$$
\frac{\partial C}{\partial X} = \frac{\partial C}{\partial S}\frac{\partial S}{\partial X} = S\frac{\partial C}{\partial S}\;.$$
Thus the term $\partial C/\partial X$ is the delta of the derivative multiplied by the price of the underlying.  Similarly, by ordinary calculus, the term $\partial^2 C/\partial X^2$
can be written in terms of the delta and the gamma of the derivative.

Sometimes one writes the derivative value as a function of time to maturity ($\tau = T-t$) instead of $t$.  The partial derivative of $C$ with respect to $\tau$ is the negative of the partial derivative with respect to $t$, so the fundamental pde is the same except for a different sign on the first term of the right-hand side of @eq-3.  Rearranging a little, we have
$$
\frac{\partial C}{\partial \tau} = -rC + \nu\frac{\partial C}{\partial X}+ \frac{1}{2}\sigma^2\frac{\partial^2 C}{\partial X^2}\;.
$$ {#eq-4}

In this form, the pde is similar to important equations in physics, in particular the equation for how heat propagates through a rod over time.  In fact, it can be transformed exactly into the heat equation, which is how Black and Scholes originally solved the option valuation problem.  The terminal condition for a call option, $C = \max(S-K,0)$, can be viewed as defining $C$ over the $X$ dimension at $\tau=0$, just as the temperature along the length of the rod might be specified at an initial date, and as $\tau$ increases $C$ changes at each point $X$ according to @eq-4, which is similar, as noted, to the equation for the change in temperature at a point on the rod as time passes.

## Discretizing the PD\\E

To numerically solve the fundamental pde, we consider a discrete grid on the $(t,x)$ space.  We label the time points as $t_0, t_1, t_2, \ldots, t_N$, and the $x$ points as $x_{-M}, x_{-M+1}, \ldots, x_0, x_1, \ldots, x_M$, with $t_0=0$, $t_N=T$, and $x_0=\log S_0$.  The equation should hold for $-\infty< x < \infty$, but obviously we will have to bound this space, and we have denoted the upper and lower bounds by $x_M$ and $x_{-M}$ here.  We take the points to be evenly spaced and set $\Delta t= t_i-t_{i-1}$ and $\Delta x = x_j -x_{j-1}$ for any $i$ and $j$.

For specificity, we will consider a call option, though the discussion in this section applies to any derivative.  We will compute a value for the call at each of the points on the grid.  Then we return the value of the call at the point $(t_0,x_0)$.  

Consider a point $(t_i, x_j)$.  We could denote the estimated value of the call at this point by $C_{ij}$ but for now we will just use the symbol $C$.  Think of $t$ being on the horizontal axis and $x$ on the vertical axis.  There are four points that can be reached from $(t_i,x_j)$ by one step (an increase or decrease) in either $t$ or $x$.  Let's denote the estimated call value at $(t_i, x_j+\Delta x)$ as $C_{\text{up}}$, the value at $(t_i,x_j-\Delta x)$ as $C_{\text{down}}$, the value at $(t_i+\Delta t, x_j)$ as $C_{\text{right}}$ and the value at $(t_i-\Delta t,x_j)$ as $C_{\text{left}}$.

We want to force  @eq-3 to hold on the grid.  To estimate $\partial C/\partial X$ and $\partial^2 C/\partial X^2$, we make exactly the same calculations we made to estimate deltas and gammas in a binomial model.  At the point $(t_i,x_j)$, we estimate

$$
\frac{\partial C}{\partial X} \approx \frac{C_{\text{up}}-C_{\text{down}}}{2\Delta x}\; .
$$ {#eq-pdedelta}

There are two other obvious estimates of this derivative:
$$
\frac{C_{\text{up}}-C}{\Delta x} \qquad \text{and} \qquad \frac{C-C_{\text{down}}}{\Delta x}\;.
$$
The first of these should be understood as an estimate at the midpoint of $x_j$ and $x_j+\Delta x$ and the second as an estimate at the midpoint of $x_j$ and $x_j-\Delta x$.  The distance between these two midpoints is $\Delta x$, so the difference in these two estimates of $\partial C/\partial X$ divided by $\Delta x$ is an estimate of the second derivative:
$$
\frac{\partial^2 C}{\partial X^2} \approx \frac{C_{\text{up}}-2C+C_{\text{down}}}{(\Delta x)^2}\;.
$$ {#eq-pdegamma}


The obvious estimate of $\partial C/\partial t$, which is analogous to the estimate of $\partial C/\partial X$, is
$$
\frac{C_{\text{right}}-C_{\text{left}}}{2\Delta t}\;.
$$
This is not the estimate we are going to use.  The reason is that we want to solve for the call values on the grid in much the same way that we solved the binomial model---starting at the end and working backwards.  If we use the above estimate of the time derivative, then at each point $(t_i,x_j)$, @eq-3 will link the call values at times $t_{i-1}$, $t_i$ and $t_{i+1}$.  This would substantially complicate the backing up process.  However, in a sense, it is the right estimate, and the Crank-Nicolson method to be discussed below uses a similar idea.

The other two choices for estimating $\partial C/\partial t$ are analogous to the other two choices for estimating $\partial C/\partial X$.  We can use either

$$
\frac{\partial C}{\partial t} \approx \frac{C-C_{\text{left}}}{\Delta t}\;,
$$ {#eq-explicit_dt}

$$
$$
or
$$
\frac{\partial C}{\partial t} \approx \frac{C_{\text{right}}-C}{\Delta t}\;. 
$$ {#eq-implicit_dt}



Using the first is called the explicit method of solving the pde, and using the second is called the implicit method.  The reason for these names should become clear below.

## Explicit and Implicit Methods

We first consider the explicit method.  \index{explicit method}We set the value of the call at the final date $t_N$ and each point $x_j$ to be its intrinsic value, $\max\left(\mathrm{e}^{x_j}-K,0\right)$.  Now consider calculating the value at date $t_{N-1}$ and any point $x_j$.  We do this by forcing  the approximation to @eq-3 based on @eq-pdedelta--@eq-explicit_dt to hold at the point $(t_N,x_j)$.  Using the same notation as before, for $(t_i,x_j)=(t_N,x_j)$, implies
$$
rC = \frac{C-C_{\text{left}}}{\Delta t}+ \nu\left(\frac{C_{\text{up}}-C_{\text{down}}}{2\Delta x}\right)+ \frac{1}{2}\sigma^2\left(\frac{C_{\text{up}}+C_{\text{down}}-2C}{(\Delta x)^2}\right)\;.
$$ {#eq-explicit}

Given that $t_i$ is the final date $t_N$, the values $C$, $C_{\text{up}}$ and $C_{\text{down}}$ have already been calculated as the intrinsic value of the call at maturity.  The only unknown is $C_{\text{left}}$, which is the value of the call at $(t_{N-1},x_j)$.  We can solve this explicitly  for $C_{\text{left}}$, whence the name of the algorithm.  We do this at each point $x_j$ at date $t_{N-1}$ (except for the top and bottom points, which we will discuss below) and then we follow the same procedure to back up sequentially to the initial date, as in the binomial model.

@eq-explicit cannot be used to find $C_{\text{left}}$ at the bottom point $x_{-M}$, because at this point there is no $C_{\text{down}}$ at date $t_N$.  Similarly, we cannot use it to find $C_{\text{left}}$ at the top point $x_M$, because at that point there is no $C_{\text{up}}$.  We have to define the values along the top and bottom of the grid in some other fashion.  We do this using conditions the derivative is known to satisfy as the stock price approaches $+\infty$ or 0.  For example, for a European call option, we use the conditions that $\partial C/\partial S \rightarrow 1$ as $S \rightarrow \infty$ and $\partial C/\partial S \rightarrow 0$ as $S \rightarrow 0$.  We will explain this in more detail in the following section.

The solution of @eq-explicit for $C_{\text{left}}$ can be written as
$$
C_{\text{left}} = \big(1-r\Delta t\big)\big(p_uC_{\text{up}}+pC + p_dC_{\text{down}}\big)\;,
$$ {#eq-fdtrinomial}

where
\begin{align*}
p_u &= \frac{\sigma^2\Delta t+\nu\Delta t\Delta x}{2(1-r\Delta t)(\Delta x)^2}\; ,\\
p_d &= \frac{\sigma^2\Delta t-\nu\Delta t\Delta x}{2(1-r\Delta t)(\Delta x)^2}\; ,\\
p &= 1- p_u-p_d\;.
\end{align*}
This can be interpreted as discounting the probability-weighted values of the call at the next date, where we consider that starting at the grid point $(t_{i},x_j)$, the logarithm of the stock price takes three possible values ($x_j-\Delta x$, $x_j$, and $x_j+\Delta x$) at the next date $t_{i+1}$, and where we use $1-r\Delta t$ as the discount factor. Thus, it is essentially a trinomial model.  \index{trinomial model}This relationship was first noted by Brennan and Schwartz [@BrennanSchwartz]. 

Actually, for this to be a sensible trinomial model, the probabilities $p_u$, $p$ and $p_d$ should be nonnegative.  Assuming $1-r\Delta t>0$, this will be the case if and only if
$$
\Delta x \leq  \frac{\sigma^2}{|\nu|} \qquad \text{and} \qquad \Delta t \leq \frac{(\Delta x)^2}{\sigma^2 + r(\Delta x)^2}\;.
$$
The first of these conditions characterizes $p_u$ and $p_d$ being nonnegative.  The second is derived from $p_u+p_d \leq 1$.  It is interesting to examine these conditions in terms of the number $N$ of time periods and the number of steps in the $x$ dimension, which is $2M$.  To simplify the notation in the following somewhat, denote the distance of the upper $x$ boundary from $x_0$ by $D$ (i.e., $D=x_M-x_0$).  Then $\Delta t=T/N$ and $\Delta x = D/M$.  The probabilities are nonnegative if and only if
$$
M \geq \frac{|\nu| D}{\sigma^2} \qquad \text{and} \qquad N \geq rT + \left(\frac{\sigma^2T}{D^2}\right)M^2\;.
$$
Consider fixing $D$ and increasing the number of time periods and space steps (i.e., steps along the $x$ dimension).  To maintain positive probabilities, the above shows that the number of time periods must increase as the square of the number of space steps: increasing $M$ by a factor of 10 requires increasing $N$ by a factor of 100.  The upshot is it can be computationally expensive to use a large number of space steps, if we want to maintain nonnegative probabilities.

One can reasonably ask whether this is important, because we can certainly solve  @eq-explicit to estimate the call values even when the probabilities are negative.  The answer is that it is important, but for a reason we have not yet discussed.  In a numerical algorithm for solving a partial differential equation (or for solving many other types of problems) there are two types of errors: discretization error and roundoff error.   If we increase $N$ and $M$ sufficiently, we should reduce the discretization error.  However, each calculation on the computer introduces roundoff error.  An algorithm is said to be stable if the \index{stable algorithm} roundoff errors stay small and bounded as the discretization error is reduced.  An unfortunate fact about the explicit method is that it is stable only if the number of time steps increases with the square of the number of space steps.  In the absence of this condition, the roundoff errors can accumulate and prevent one from reaching a solution of the desired accuracy.  

The implicit method is known to be fully stable, so it is to be preferred to the explicit method.  We will discuss briefly how to implement this method, before moving in the next section to the Crank-Nicolson method, which is also fully stable and known to be more efficient than the implicit method.

The implicit method \index{implicit method} uses the approximation @eq-implicit_dt for $\partial C/\partial t$.  As before, the call values are defined at the final date as the intrinsic value.  Backing up a period, consider a grid point $(t_{N-1},x_j)$.  We will try to estimate the call value at this date by forcing @eq-3 to hold at this point.  This means
$$
rC = \frac{C_{\text{right}}-C}{\Delta t}+ \nu\left(\frac{C_{\text{up}}-C_{\text{down}}}{2\Delta x}\right)+ \frac{1}{2}\sigma^2\left(\frac{C_{\text{up}}+C_{\text{down}}-2C}{(\Delta x)^2}\right)\;.
$$ {#eq-implicit}

We know $C_{\text{right}}$, because it is the intrinsic value at $(t_N,x_j)$.  This equation links three unknowns ($C$, $C_{\text{up}}$, and $C_{\text{down}}$) to the known value $C_{\text{right}}$.  We cannot solve it explicitly for these three unknowns.  Instead, we need to solve a system of linear equations to simultaneously solve for all the call values at  date $t_{N-1}$.  There are $2M-1$ equations of the form @eq-implicit plus conditions that we will impose at the upper and lower boundaries, and we need to solve these for the $2M+1$ call values.  This system of equations has the same form, and is solved in the same way, as the system of equations in the Crank-Nicolson method.

## Crank-Nicolson
\index{Crank-Nicolson method}
The estimate @eq-implicit_dt of $\partial C/\partial t$ used in the implicit method is best understood as an estimate of 
$\partial C/\partial t$ at the midpoint of $(t_i,x_j)$ and $(t_{i+1},x_j)$, i.e., at $(t_i+\Delta t/2,x_j)$.  This is the basic idea of the Crank-Nicolson method.  With this method, we continue to estimate the call values at the grid points, but we do so by forcing  @eq-3 to hold at midpoints of this type.  To do this, we also need estimates of $C$, $\partial C/\partial X$ and $\partial^2 C/\partial X^2$ at the midpoints, but these are easy to obtain.

Let's modify the previous notation somewhat, writing $C'$ for $C_{\text{right}}$ and $C'_{\text{up}}$ and $C'_{\text{down}}$ for the values to the right and one step up and down, i.e., at the grid points $(t_i+\Delta t,x_i+\Delta x)$ and $(t_i+\Delta t,x_i-\Delta x)$ respectively.  The obvious estimate of the call value at the midpoint $(t_i+\Delta t/2,x_j)$ is the average of $C$ and $C'$, so set
$$
C^{\text{mid}} = \frac{C+C'}{2}\;.
$$
Analogously, define
$$
C^{\text{mid}}_{\text{up}} = \frac{C_{\text{up}}+C'_{\text{up}}}{2}\;, \qquad \text{and} \qquad
C^{\text{mid}}_{\text{down}} = \frac{C_{\text{down}}+C'_{\text{down}}}{2}\;.
$$ {#eq-cn1000}

The formulas @eq-cn1000 give us estimates of the call value at the midpoints one space step up and one space step down from $x_j$---i.e., at $(t_i+\Delta t/2,x_{j+1})$ and $(t_i+\Delta t/2,x_{j-1})$.  We can now estimate
$\partial C/\partial X$ and $\partial^2 C/\partial X^2$ at the midpoint $(t_i+\Delta t/2,x_j)$ exactly as before:
$$
\frac{\partial C}{\partial X} \approx \frac{C^{\text{mid}}_{\text{up}}-C^{\text{mid}}_{\text{down}}}{2\Delta x}\;,
$$
and
$$
\frac{\partial^2 C}{\partial X^2} \approx \frac{C^{\text{mid}}_{\text{up}}+C^{\text{mid}}_{\text{down}}-2C^{\text{mid}}}{(\Delta x)^2}\;.
$$
Now,  @eq-3 becomes
$$
rC^{\text{mid}} = \frac{C'-C}{\Delta t}+ \nu\left(\frac{C^{\text{mid}}_{\text{up}}-C^{\text{mid}}_{\text{down}}}{2\Delta x}\right)+ \frac{1}{2}\sigma^2\left(\frac{C^{\text{mid}}_{\text{up}}+C^{\text{mid}}_{\text{down}}-2C^{\text{mid}}}{(\Delta x)^2}\right)\;.
$$ {#eq-crank}

Substituting from the formulas for $C^{\text{mid}}$, $C^{\text{mid}}_{\text{up}}$, and $C^{\text{mid}}_{\text{down}}$, we can re-write @eq-crank as

$$
\begin{multline}
\left(\frac{r}{2}+\frac{1}{\Delta t}+\frac{\sigma^2}{2(\Delta x)^2}\right)C - \left(\frac{\sigma^2}{4(\Delta x)^2}+\frac{\nu}{4\Delta x}\right)C_{\text{up}}\\ - \left(\frac{\sigma^2}{4(\Delta x)^2}-\frac{\nu}{4\Delta x}\right)C_{\text{down}} 
\quad = \quad \left(\frac{1}{\Delta t}- \frac{r}{2}-\frac{\sigma^2}{2(\Delta x)^2}\right)C' \\+ \left(\frac{\sigma^2}{4(\Delta x)^2}+\frac{\nu}{4\Delta x}\right)C'_{\text{up}} + \left(\frac{\sigma^2}{4(\Delta x)^2}-\frac{\nu}{4\Delta x}\right)C'_{\text{down}}
\end{multline}
$$ {#eq-crank2}

We can also write this as
$$
a_1C - a_2C_{\text{up}} - a_3C_{\text{down}} 
= a_4C' + a_2C'_{\text{up}} + a_3C'_{\text{down}}\;,$$ {#eq-crank3}

where the constants $a_i$ are the factors in parentheses in @eq-crank2.

As before, we start at the final date $t_N$ and define the call value at that date by its intrinsic value.  Consider a grid point $(t_{N-1},x_j)$.  Forcing  @eq-3 to hold at the midpoint $(t_{N-1}+\Delta t/2,x_j)$ leads us to @eq-crank3.  In this equation, $C'$, $C'_{\text{up}}$ and $C'_{\text{down}}$ are known from the intrinsic value at maturity, and we need to solve for $C$, $C_{\text{up}}$ and $C_{\text{down}}$.  There are $2M-1$ linear equations of this type and we will add linear equations at the upper and lower boundaries of the grid and solve the resulting system of $2M+1$ linear equations for the $2M+1$ call values.  After finding the call values at date $t_{N-1}$, we then repeat the calculation at $t_{N-2}$ and continue backing up in this way until we reach the initial date.

Notice that @eq-crank2 is similar to @eq-implicit in the implicit method, but more information is used in each step of the Crank-Nicolson method than is used in each step of the implicit method.  @eq-crank2 links the call values $C$, $C_{\text{up}}$ and $C_{\text{down}}$ to the previously calculated $C'$, $C'_{\text{up}}$ and $C'_{\text{down}}$, whereas in the implicit method they were linked only to $C'$ (which we called $C_{\text{right}}$).


## European  Options

To value a European option, one simply defines the values at the final date as the intrinsic value and then backs up to the initial date, using any of the methods described (explicit, implicit, or Crank-Nicolson).  The value that should be returned is the value at the middle node at the initial date, which corresponds to the initial price of the underlying.

The boundary conditions normally used at the bottom and top of the grid are conditions that the first derivative $\partial C/\partial S$ of the option value  are known to satisfy as $S \rightarrow 0$ and $S \rightarrow \infty$.  These are conditions of the form
$$
\lim_{S \rightarrow \infty} \frac{\partial C}{\partial S} =  \lambda_0\;,\qquad \text{and} \qquad
\lim_{S \rightarrow 0} \frac{\partial C}{\partial S} = \lambda_{\infty}\;,
$$ {#eq-cn_boundary}

for constants $\lambda_0$ and $\lambda_{\infty}$.
In the case of a call option, we have $\lambda_0=0$ and $\lambda_{\infty}=1$.  For a put option, we have $\lambda_0=-1$ and $\lambda_\infty = 0.$

These conditions are implemented on the grid by forcing each value $C$ at a point $(t_i,x_{-M})$ on the bottom of the grid to satisfy


$$
C - C_{\text{up}} = \lambda_0 (S - S_{\text{up}})
$$ {#eq-crank4a}

and by forcing each value $C$ at a point $(t_i,x_{M})$ on the top of the grid to satisfy

$$
C-C_{\text{down}} = \lambda_\infty(S-S_{\text{down}})\;.
$$ {#eq-crank4b}



These two linear equations in the values at time $t_i$ augment the $2M-1$ equations already described to form a system of $2M+1$ linear equations to be solved for the derivative values at the $2M+1$ grid points at time $t_i$. 

We will create a program that solves a system of equations of the form @eq-crank3, @eq-crank4a and @eq-crank4b.  We input the vector $a$ of coefficients, a vector $y$ of dimension $2M+1$ containing the estimated values of the derivative at any date $t_{i+1}$, an integer $L$ from which $M$ is defined as $M=(L-1)/2$ (i.e., $L=2M+1$).  The function will return the vector of values at date $t_i$.  

We will write the boundary conditions @eq-crank4a and @eq-crank4b, respectively, in the more general forms

$$
C = z_1 + b_1C_{\text{up}}\;,
$$ {#eq-crank4aa}

and
$$
C = z_L + b_LC_{\text{down}}\;,
$$ {#eq-crank4bb}


where $z_1$, $b_1$, $z_L$ and $b_L$ are numbers to be calculated or input by the user.
  @eq-crank4a and @eq-crank4b are the special cases in which  $z_1 = \lambda_0(S-S_{\text{up}})$, $b_1 = 1$, $z_L = \lambda_\infty(S-S_{\text{down}})$,  and $b_L=1$.   The additional generality in allowing $b_1$ and $b_L$ to be different from one is important for many purposes, and we will see an example of it in the valuation of barrier options.  

The system of equations that we want to solve is therefore 
\setcounter{MaxMatrixCols}{11}

\begin{pmatrix} 
1 & -b_1 & 0 & 0 & 0  & \cdots  & 0 & 0 & 0 & 0 & 0\\
-a_3 & a_1 & - a_2 & 0 & 0 & \cdots & 0 & 0 & 0 & 0 & 0\\
0 & - a_3 & a_1 & -a_2 & 0 & \cdots & 0 & 0 & 0 & 0 & 0\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots &\vdots & \vdots & \vdots & \vdots & \vdots \\
0 & 0 & 0 & 0 & 0 & \cdots & 0 & -a_3 & a_1 & - a_2 & 0\\
0 & 0 & 0 & 0 & 0 & \cdots  & 0 & 0 & -a_3 & a_1 & - a_2 \\
0 & 0 & 0 & 0 & 0 & \cdots  & 0 & 0 & 0 & -b_L & 1 
\end{pmatrix}

\begin{pmatrix}
C_1 \\
C_2\\
C_3\\
\vdots\\
C_{L-2}\\
C_{L-1}\\
C_L
\end{pmatrix}
\begin{matrix}
\phantom\\
\phantom\\
\phantom\\
= 
\phantom\\
\phantom\\
\phantom\\
\end{matrix}
\begin{pmatrix}
z_1 \\
z_2\\
z_3\\
\vdots\\
z_{L-2}\\
z_{L-1}\\
z_L
\end{pmatrix}



where we are denoting the derivative values to be determined at date $t_i$ across the $L$ ($=2M+1$) space nodes as $C_1, \ldots, C_L$.  The coefficients $a_i$ are defined in  @eq-crank3.  The numbers $z_2, \ldots, z_{L-1}$ are the right-hand sides of  @eq-crank3 and are determined by the coefficients $a_i$ and the derivative values $y_1, \ldots, y_{L}$ at date $t_{i+1}$.  The system of equations that must be solved to implement the implicit method is of this same form.

The first equation in this array (@eq-crank4aa) can be written as

$$
C_1 = u_1 + b_1C_2\;,
$$
where $u_1 = z_1$.   
By induction, we will see that we can write, for each $j = 2, \ldots, L$,
$$
C_{j-1} = u_{j-1} + b_{j-1}C_{j}
$$ {#eq-crank101}

for some coefficients $u_{j-1}$ and $b_{j-1}$ to be determined.  
The $j$--th equation ($j = 2, \ldots, L-1$) in the array (@eq-crank3)  is 
$$-a_3C_{j-1}+a_1C_j-a_2C_{j+1} = z_j\; .$$
Supposing @eq-crank101 holds and using it to substitute for $C_{j-1}$, we have
$$-a_3\left(u_{j-1} + b_{j-1}C_{j}\right) +a_1C_j-a_2C_{j+1} = z_j \quad\Longleftrightarrow\quad C_{j} = u_{j} + b_{j}C_{j+1}\; ,$$
where
\begin{align*}
u_j &= \frac{z_j + a_3u_{j-1}}{a_1-a_3b_{j-1}} \;,\\
b_j &= \frac{a_2}{a_1-a_3b_{j-1}}\;.
\end{align*}
This establishes that @eq-crank101 holds for each $j = 2, \ldots, L$.

The last equation in the array (@eq-crank4bb) is 
$$
C_L = z_L + b_LC_{L-1}\; .$$ {#eq-crankC_L}
Our induction argument gives us
$$
C_{L-1} = u_{L-1} + b_{L-1}C_{L}\; ,
$$
and when we combine these we have two equations in two unknowns and can solve for $C_L$ as
$$
C_L = \frac{z_L+ b_Lu_{L-1}}{1-b_Lb_{L-1}}\;.
$$
We then successively obtain $C_{L-1}, C_{L-2}, \ldots, C_1$ from  @eq-crank101.

We will demonstrate the Crank-Nicolson method by valuing a European call.  Any other path-independent European derivative is valued in the same way, by appropriately redefining the value of the derivative at the final date and redefining the constants $z_1$ and $z_L$ in the boundary conditions @eq-crank4aa - @eq-crank4bb at the bottom and top of the grid.  

As elsewhere in this chapter, $N$ denotes the number of time periods, and $2M+1$ will be the number of $x$ values on the grid.  We use the symbol $D$ to denote the distance of the top (or bottom) of the grid from $\log S_0$.  In other words, $D = x_M-x_0$.  A reasonable value for $D$ would be three standard deviations for $\log S$, which would mean $D = |\nu|T +3 \sigma\sqrt{T}$. For example, for a one-year option on a stock with a volatility of 30\%, it should suffice to input $D=1$.  

As should be clear, the program is conceptually very similar to a binomial model.  The difference is that the backing up procedure, which involves node-by-node discounting in a binomial model, here is accomplished via the Crank-Nicolson algorithm.^[We use a different variable ($y$) for the call values at the final date---and consequently need to separate the first step of backing up (to the penultimate date) and the other steps of backing up (to date zero).  See Appendix~A for more discussion.]  

The following code illustrates how to  use the Crank_Nicolson method to vale a European call option. 

```{python}

#| code-fold: true
#| label: Crank_Nicolson

import numpy as np

def crank_nicolson(a, y, L, z1, b1, zL, bL):
    u = np.zeros(L)
    b = np.zeros(L)
    c = np.zeros(L)
    z = np.zeros(L)

    u[0] = z1
    b[0] = b1
    for j in range(1, L - 1):
        z[j] = a[3] * y[j] + a[1] * y[j + 1] + a[2] * y[j - 1]
        u[j] = (a[2] * u[j - 1] + z[j]) / (a[0] - a[2] * b[j - 1])
        b[j] = a[1] / (a[0] - a[2] * b[j - 1])
    c[-1] = (zL + bL * u[-2]) / (1 - bL * b[-2])
    for j in range(L - 2, -1, -1):
        c[j] = u[j] + b[j] * c[j + 1]
    return c

def european_call_crank_nicolson(S0, K, r, sigma, q, T, N, M, Dist):
    dt = T / N
    dx = Dist / M
    dx2 = dx ** 2
    u = np.exp(dx)
    sig2 = sigma ** 2
    nu = r - q - sig2 / 2
    St = S0 * np.exp(Dist)
    Sb = S0 * np.exp(-Dist)
    a = np.zeros(4)
    a[0] = r / 2 + 1 / dt + sig2 / (2 * dx2)
    a[1] = sig2 / (4 * dx2) + nu / (4 * dx)
    a[2] = a[1] - nu / (2 * dx)
    a[3] = -a[0] + 2 / dt

    L = 2 * M + 1
    y = np.zeros(L)
    S = Sb
    y[0] = max(S - K, 0)
    for j in range(1, L):
        S *= u
        y[j] = max(S - K, 0)

    z1 = 0
    b1 = 1
    zL = St - St / u
    bL = 1
    CallV = crank_nicolson(a, y, L, z1, b1, zL, bL)

    for _ in range(N - 2, -1, -1):
        CallV = crank_nicolson(a, CallV, L, z1, b1, zL, bL)
    return CallV[M]

# Example usage
S0 = 100
K = 90
r = 0.05
sigma = 0.2
q = 0.02
T = 1
N = 100
M = 50
Dist = 3
Bar = 85

print("European Call Crank-Nicolson:", european_call_crank_nicolson(S0, K, r, sigma, q, T, N, M, Dist))
```

## American Options

The explicit method is easily adapted to American options.  As in a binomial model, we compute the option value at each node as the larger of its discounted expected value and its intrinsic value.  To be somewhat more precise, we replace the trinomial value @eq-fdtrinomial with
$$
C_{\text{left}} = \max\left(\big(1-r\Delta t\big)\big(p_uC_{\text{up}}+pC + p_dC_{\text{down}}\big), \text{intrinsic value}\right)\;.
$$

In the Crank-Nicolson method, one can in similar fashion compute the value of the derivative at each space node at any date by solving the system of equations @eq-crank3 and then replace the computed values by the intrinsic value when that is higher.  However, because the values at the different space nodes are linked (i.e., the method is an implicit-type method), this one-at-a-time replacement of values by intrinsic values is not the most efficient method.  See Wilmott [@Wilmott] for more details (and for VBA code implementing the projected successive over-relaxation method).

## Barrier Options {#sec-s:finitedifferencebarriers}


\index{barrier option}\index{down-and-out option}\index{down-and-in option}\index{knock-out option}\index{knock-in option}Finite-difference methods work well for valuing discretely-sampled barrier options.  For a down-and-out option, one should place the bottom of the grid at the knock-out boundary.  For an up-and-out option, one should place the top of the grid at the knock-out boundary.  As discussed in @sec-c:exotics, knock-in options can be valued as standard options minus knock-out options.  

For barrier options, the boundary information @eq-cn_boundary can be replaced by assigning a value of zero at the knock-out boundary.  For example, for a down-and-out option, the condition @eq-crank4a can be replaced by $C=0$.  If the contract specifies that a rebate is to be paid to the buyer when the option is knocked out, then condition @eq-crank4a should be replaced by $C=$ Rebate.  

To price a down-and-out (or up-and-out option), we put the bottom (or top) of the grid at the boundary.  The boundary condition that we use is that the value at the boundary is zero.   We will consider the example of a down-and-out call option.  In this case, the boundary condition at the bottom of the grid is @eq-crank4aa with $z_1=0$ and $b_1 = 0$.  The boundary condition at the top is the same as for an ordinary call.  We can easily handle a rebate paid when the option is knocked out by inputting the value of the rebate as $z_1$.  

The main new issue that we encounter in valuing barriers is locating the boundary of the grid at the barrier.  For the down-and-out, we will input the value of the stock price at which the option is knocked out as `Bar`.  We want the bottom of the grid to lie at the natural logarithm of this number.  This will influence our choice of the space step $\Delta x$, because we want to have an integer number of steps between the bottom of the grid and $\log S_0$.  We assume that the value $M$ input by the user represents the desired number of space steps above $\log S_0$.  We start with $\Delta x=D/M$ as an initial estimate of the size of the space step.  We then decrease it, if necessary, to ensure that the distance between `Bar` and $\log S_0$ is an integer multiple of $\Delta x$.  We then increase $M$, if necessary, to ensure that the top of the grid will still be at or above $D+\log S_0$.  Finally, we define the top of the grid to be at $\log S_0 +M \cdot \Delta x$.  

```{python}

#| code-fold: true
#| label: my_code_block

def down_and_out_call_cn(S0, K, r, sigma, q, T, N, M, Dist, Bar):
    dx = Dist / M
    DistBot = np.log(S0) - np.log(Bar)
    NumBotSteps = int(np.ceil(DistBot / dx))
    dx = DistBot / NumBotSteps
    NumTopSteps = int(np.ceil(Dist / dx))
    DistTop = NumTopSteps * dx
    L = NumBotSteps + NumTopSteps + 1
    dt = T / N
    dx2 = dx ** 2
    u = np.exp(dx)
    sig2 = sigma ** 2
    nu = r - q - sig2 / 2
    St = S0 * np.exp(DistTop)
    a = np.zeros(4)
    a[0] = r / 2 + 1 / dt + sig2 / (2 * dx2)
    a[1] = sig2 / (4 * dx2) + nu / (4 * dx)
    a[2] = a[1] - nu / (2 * dx)
    a[3] = -a[0] + 2 / dt

    y = np.zeros(L)
    S = Bar
    y[0] = max(S - K, 0)
    for j in range(1, L):
        S *= u
        y[j] = max(S - K, 0)

    z1 = 0
    b1 = 0
    zL = St - St / u
    bL = 1
    CallV = crank_nicolson(a, y, L, z1, b1, zL, bL)

    for _ in range(N - 2, -1, -1):
        CallV = crank_nicolson(a, CallV, L, z1, b1, zL, bL)
    return CallV[NumBotSteps]

print("Down and Out Call CN:", down_and_out_call_cn(S0, K, r, sigma, q, T, N, M, Dist, Bar))

```



## {.unnumbered}

::: Exercise
 Create a python program to compare the estimates of the value of a discretely sampled barrier option given by the functions Down_And_Out_Call_MC created before and the function Down_And_Out_Call_CN.  Allow the user to input $S$, $K$, $r$, $\sigma$, $q$, the knock-out barrier, the number of Monte Carlo simulations, and the number of space steps above $\log S_0$ in the Crank-Nicolson algorithm.  
:::

::: Exercise
 Create a python function Up_And_Out_Put_CN to value an up-and-out put option by the Crank-Nicolson method.
:::
::: Exercise
Create a python function European_Call_Explicit that uses the explicit method @eq-fdtrinomial to value a European call option.
:::

::: Exercise

Write  @eq-implicit for the implicit method, together with boundary conditions of the form @eq-crank4aa - @eq-crank4bb as a matrix system and solve for $u_j$ and $b_j$ in @eq-crank101 - @eq-crankC_L, as in the subsection that defines the function CrankNicolson.
:::

::: Exercise
Create a python function Implicit that solves the system of equations in the preceding exercise.
:::
::: Exercise

Create a python function European_Call_Implicit that uses the implicit method to value a European call option.
:::
